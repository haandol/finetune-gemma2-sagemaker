{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab8b4a5c-8198-486a-86ed-a6cf048ba8c4",
   "metadata": {},
   "source": [
    "# 1. Setup Development Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd66d83d-af69-4edd-8cad-1020f78d62e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "!pip install -q -U trl\n",
    "!pip install -q -U sagemaker\n",
    "!pip install -q -U \"datasets[s3]\"\n",
    "!pip install -q -U \"huggingface_hub[cli]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1254a7c0-9fba-415d-b4e3-ca5d907786a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: datasets\n",
      "Version: 2.20.0\n",
      "Summary: HuggingFace community-driven open-source library of datasets\n",
      "Home-page: https://github.com/huggingface/datasets\n",
      "Author: HuggingFace Inc.\n",
      "Author-email: thomas@huggingface.co\n",
      "License: Apache 2.0\n",
      "Location: /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages\n",
      "Requires: aiohttp, dill, filelock, fsspec, huggingface-hub, multiprocess, numpy, packaging, pandas, pyarrow, pyarrow-hotfix, pyyaml, requests, tqdm, xxhash\n",
      "Required-by: trl\n",
      "Name: pandas\n",
      "Version: 1.5.3\n",
      "Summary: Powerful data structures for data analysis, time series, and statistics\n",
      "Home-page: https://pandas.pydata.org\n",
      "Author: The Pandas Development Team\n",
      "Author-email: pandas-dev@python.org\n",
      "License: BSD-3-Clause\n",
      "Location: /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages\n",
      "Requires: numpy, python-dateutil, pytz\n",
      "Required-by: autovizwidget, bokeh, datasets, hdijupyterutils, nvgpu, sagemaker, seaborn, shap, smclarify, sparkmagic, statsmodels\n"
     ]
    }
   ],
   "source": [
    "!pip show datasets\n",
    "!pip show pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fb90b63-d43d-43cb-816a-847a001c436e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "161c2985671f48fa88089910af8afea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34eed579-7ba7-4f0a-a6e7-5aeadd64b8a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::395271362395:role/SagemakerStudioDemoSagema-SageMakerExecutionRole78-5I33I083KE6P\n",
      "sagemaker bucket: sagemaker-us-east-1-395271362395\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"AmazonSageMaker-ExecutionRole-20230112T181165\")[\n",
    "        \"Role\"\n",
    "    ][\"Arn\"]\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bf1360-9fd5-401f-9eac-81f6d0ebcac2",
   "metadata": {},
   "source": [
    "# 2. Create and prepare the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a45f0aa1-3427-446a-b9ac-225692cd6f3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'sql', 'sql_explanation'],\n",
       "    num_rows: 12500\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"gretelai/synthetic_text_to_sql\", split=\"train\")\n",
    "dataset = dataset.shuffle(seed=1234).select(range(12500))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af4c82e2-992c-4db1-9088-5cc469dd4411",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>domain</th>\n",
       "      <th>domain_description</th>\n",
       "      <th>sql_complexity</th>\n",
       "      <th>sql_complexity_description</th>\n",
       "      <th>sql_task_type</th>\n",
       "      <th>sql_task_type_description</th>\n",
       "      <th>sql_prompt</th>\n",
       "      <th>sql_context</th>\n",
       "      <th>sql</th>\n",
       "      <th>sql_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65582</td>\n",
       "      <td>disability services</td>\n",
       "      <td>Comprehensive data on disability accommodation...</td>\n",
       "      <td>basic SQL</td>\n",
       "      <td>basic SQL with a simple select statement</td>\n",
       "      <td>data manipulation</td>\n",
       "      <td>inserting, updating, or deleting records</td>\n",
       "      <td>Update the budget for the 'ASL Interpreter' se...</td>\n",
       "      <td>CREATE TABLE Regions (RegionID INT, RegionName...</td>\n",
       "      <td>UPDATE SupportServices SET Budget = 16000 WHER...</td>\n",
       "      <td>This query updates the budget for the 'ASL Int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83180</td>\n",
       "      <td>climate change</td>\n",
       "      <td>Climate change data on climate mitigation, cli...</td>\n",
       "      <td>basic SQL</td>\n",
       "      <td>basic SQL with a simple select statement</td>\n",
       "      <td>analytics and reporting</td>\n",
       "      <td>generating reports, dashboards, and analytical...</td>\n",
       "      <td>Find the intersection of mitigation and adapta...</td>\n",
       "      <td>CREATE TABLE mitigation (id INT PRIMARY KEY, c...</td>\n",
       "      <td>SELECT m.action FROM mitigation m, adaptation ...</td>\n",
       "      <td>This SQL query identifies the intersection of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90518</td>\n",
       "      <td>marine biology</td>\n",
       "      <td>Comprehensive data on marine species, oceanogr...</td>\n",
       "      <td>basic SQL</td>\n",
       "      <td>basic SQL with a simple select statement</td>\n",
       "      <td>analytics and reporting</td>\n",
       "      <td>generating reports, dashboards, and analytical...</td>\n",
       "      <td>List all marine species with a conservation st...</td>\n",
       "      <td>CREATE TABLE species (id INT, name VARCHAR(255...</td>\n",
       "      <td>SELECT name FROM species WHERE conservation_st...</td>\n",
       "      <td>The SQL query filters the species table for ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42346</td>\n",
       "      <td>rural development</td>\n",
       "      <td>Agricultural innovation metrics, rural infrast...</td>\n",
       "      <td>basic SQL</td>\n",
       "      <td>basic SQL with a simple select statement</td>\n",
       "      <td>analytics and reporting</td>\n",
       "      <td>generating reports, dashboards, and analytical...</td>\n",
       "      <td>Find the minimum budget for agricultural innov...</td>\n",
       "      <td>CREATE TABLE agricultural_innovation (id INT, ...</td>\n",
       "      <td>SELECT MIN(budget) FROM agricultural_innovation;</td>\n",
       "      <td>The SQL query calculates the minimum budget fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86672</td>\n",
       "      <td>retail</td>\n",
       "      <td>Retail data on circular supply chains, ethical...</td>\n",
       "      <td>single join</td>\n",
       "      <td>only one join (specify inner, outer, cross)</td>\n",
       "      <td>analytics and reporting</td>\n",
       "      <td>generating reports, dashboards, and analytical...</td>\n",
       "      <td>What is the maximum price of a product sold by...</td>\n",
       "      <td>CREATE TABLE vendors(vendor_id INT, vendor_nam...</td>\n",
       "      <td>SELECT MAX(transactions.price) FROM transactio...</td>\n",
       "      <td>The SQL query calculates the maximum price of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>65425</td>\n",
       "      <td>arctic research</td>\n",
       "      <td>In-depth data on climate change, biodiversity,...</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>aggregation functions (COUNT, SUM, AVG, MIN, M...</td>\n",
       "      <td>analytics and reporting</td>\n",
       "      <td>generating reports, dashboards, and analytical...</td>\n",
       "      <td>What is the average temperature recorded in ea...</td>\n",
       "      <td>CREATE TABLE WeatherData (Station VARCHAR(255)...</td>\n",
       "      <td>SELECT Station, AVG(Temperature) FROM WeatherD...</td>\n",
       "      <td>This SQL query calculates the average temperat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>62717</td>\n",
       "      <td>arts and culture</td>\n",
       "      <td>Audience demographics, event attendance, progr...</td>\n",
       "      <td>subqueries</td>\n",
       "      <td>subqueries, including correlated and nested su...</td>\n",
       "      <td>data manipulation</td>\n",
       "      <td>inserting, updating, or deleting records</td>\n",
       "      <td>Delete records of artists who have not partici...</td>\n",
       "      <td>CREATE TABLE Artists (artist_id INT, artist_na...</td>\n",
       "      <td>DELETE FROM Artists WHERE artist_id NOT IN (SE...</td>\n",
       "      <td>This query deletes records of artists from the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10921</td>\n",
       "      <td>retail</td>\n",
       "      <td>Retail data on circular supply chains, ethical...</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>aggregation functions (COUNT, SUM, AVG, MIN, M...</td>\n",
       "      <td>analytics and reporting</td>\n",
       "      <td>generating reports, dashboards, and analytical...</td>\n",
       "      <td>List all suppliers that have provided both rec...</td>\n",
       "      <td>CREATE TABLE suppliers (supplier_id INT, suppl...</td>\n",
       "      <td>SELECT supplier_name FROM suppliers WHERE mate...</td>\n",
       "      <td>This query identifies all suppliers that have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12727</td>\n",
       "      <td>manufacturing</td>\n",
       "      <td>Detailed records on ethical manufacturing, cir...</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>aggregation functions (COUNT, SUM, AVG, MIN, M...</td>\n",
       "      <td>analytics and reporting</td>\n",
       "      <td>generating reports, dashboards, and analytical...</td>\n",
       "      <td>What is the total number of employees working ...</td>\n",
       "      <td>CREATE TABLE ethical_manufacturing (country VA...</td>\n",
       "      <td>SELECT country, SUM(employees) as total_employ...</td>\n",
       "      <td>This query calculates the total number of empl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>73714</td>\n",
       "      <td>sports entertainment</td>\n",
       "      <td>Sports team performance data, fan demographics...</td>\n",
       "      <td>single join</td>\n",
       "      <td>only one join (specify inner, outer, cross)</td>\n",
       "      <td>analytics and reporting</td>\n",
       "      <td>generating reports, dashboards, and analytical...</td>\n",
       "      <td>List the total number of tickets sold for home...</td>\n",
       "      <td>CREATE TABLE teams (team_id INT, team_name VAR...</td>\n",
       "      <td>SELECT t.team_name, SUM(g.price * g.attendance...</td>\n",
       "      <td>Join teams and games tables, filter on games f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                domain  \\\n",
       "0  65582   disability services   \n",
       "1  83180        climate change   \n",
       "2  90518        marine biology   \n",
       "3  42346     rural development   \n",
       "4  86672                retail   \n",
       "5  65425       arctic research   \n",
       "6  62717      arts and culture   \n",
       "7  10921                retail   \n",
       "8  12727         manufacturing   \n",
       "9  73714  sports entertainment   \n",
       "\n",
       "                                  domain_description sql_complexity  \\\n",
       "0  Comprehensive data on disability accommodation...      basic SQL   \n",
       "1  Climate change data on climate mitigation, cli...      basic SQL   \n",
       "2  Comprehensive data on marine species, oceanogr...      basic SQL   \n",
       "3  Agricultural innovation metrics, rural infrast...      basic SQL   \n",
       "4  Retail data on circular supply chains, ethical...    single join   \n",
       "5  In-depth data on climate change, biodiversity,...    aggregation   \n",
       "6  Audience demographics, event attendance, progr...     subqueries   \n",
       "7  Retail data on circular supply chains, ethical...    aggregation   \n",
       "8  Detailed records on ethical manufacturing, cir...    aggregation   \n",
       "9  Sports team performance data, fan demographics...    single join   \n",
       "\n",
       "                          sql_complexity_description            sql_task_type  \\\n",
       "0           basic SQL with a simple select statement        data manipulation   \n",
       "1           basic SQL with a simple select statement  analytics and reporting   \n",
       "2           basic SQL with a simple select statement  analytics and reporting   \n",
       "3           basic SQL with a simple select statement  analytics and reporting   \n",
       "4        only one join (specify inner, outer, cross)  analytics and reporting   \n",
       "5  aggregation functions (COUNT, SUM, AVG, MIN, M...  analytics and reporting   \n",
       "6  subqueries, including correlated and nested su...        data manipulation   \n",
       "7  aggregation functions (COUNT, SUM, AVG, MIN, M...  analytics and reporting   \n",
       "8  aggregation functions (COUNT, SUM, AVG, MIN, M...  analytics and reporting   \n",
       "9        only one join (specify inner, outer, cross)  analytics and reporting   \n",
       "\n",
       "                           sql_task_type_description  \\\n",
       "0           inserting, updating, or deleting records   \n",
       "1  generating reports, dashboards, and analytical...   \n",
       "2  generating reports, dashboards, and analytical...   \n",
       "3  generating reports, dashboards, and analytical...   \n",
       "4  generating reports, dashboards, and analytical...   \n",
       "5  generating reports, dashboards, and analytical...   \n",
       "6           inserting, updating, or deleting records   \n",
       "7  generating reports, dashboards, and analytical...   \n",
       "8  generating reports, dashboards, and analytical...   \n",
       "9  generating reports, dashboards, and analytical...   \n",
       "\n",
       "                                          sql_prompt  \\\n",
       "0  Update the budget for the 'ASL Interpreter' se...   \n",
       "1  Find the intersection of mitigation and adapta...   \n",
       "2  List all marine species with a conservation st...   \n",
       "3  Find the minimum budget for agricultural innov...   \n",
       "4  What is the maximum price of a product sold by...   \n",
       "5  What is the average temperature recorded in ea...   \n",
       "6  Delete records of artists who have not partici...   \n",
       "7  List all suppliers that have provided both rec...   \n",
       "8  What is the total number of employees working ...   \n",
       "9  List the total number of tickets sold for home...   \n",
       "\n",
       "                                         sql_context  \\\n",
       "0  CREATE TABLE Regions (RegionID INT, RegionName...   \n",
       "1  CREATE TABLE mitigation (id INT PRIMARY KEY, c...   \n",
       "2  CREATE TABLE species (id INT, name VARCHAR(255...   \n",
       "3  CREATE TABLE agricultural_innovation (id INT, ...   \n",
       "4  CREATE TABLE vendors(vendor_id INT, vendor_nam...   \n",
       "5  CREATE TABLE WeatherData (Station VARCHAR(255)...   \n",
       "6  CREATE TABLE Artists (artist_id INT, artist_na...   \n",
       "7  CREATE TABLE suppliers (supplier_id INT, suppl...   \n",
       "8  CREATE TABLE ethical_manufacturing (country VA...   \n",
       "9  CREATE TABLE teams (team_id INT, team_name VAR...   \n",
       "\n",
       "                                                 sql  \\\n",
       "0  UPDATE SupportServices SET Budget = 16000 WHER...   \n",
       "1  SELECT m.action FROM mitigation m, adaptation ...   \n",
       "2  SELECT name FROM species WHERE conservation_st...   \n",
       "3   SELECT MIN(budget) FROM agricultural_innovation;   \n",
       "4  SELECT MAX(transactions.price) FROM transactio...   \n",
       "5  SELECT Station, AVG(Temperature) FROM WeatherD...   \n",
       "6  DELETE FROM Artists WHERE artist_id NOT IN (SE...   \n",
       "7  SELECT supplier_name FROM suppliers WHERE mate...   \n",
       "8  SELECT country, SUM(employees) as total_employ...   \n",
       "9  SELECT t.team_name, SUM(g.price * g.attendance...   \n",
       "\n",
       "                                     sql_explanation  \n",
       "0  This query updates the budget for the 'ASL Int...  \n",
       "1  This SQL query identifies the intersection of ...  \n",
       "2  The SQL query filters the species table for ro...  \n",
       "3  The SQL query calculates the minimum budget fo...  \n",
       "4  The SQL query calculates the maximum price of ...  \n",
       "5  This SQL query calculates the average temperat...  \n",
       "6  This query deletes records of artists from the...  \n",
       "7  This query identifies all suppliers that have ...  \n",
       "8  This query calculates the total number of empl...  \n",
       "9  Join teams and games tables, filter on games f...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataset.to_pandas()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d13c7869-9804-4fa9-869c-ecd1a5ed8355",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_prompt(datum):\n",
    "    prompt = f\"\"\"\n",
    "<start_of_turn>user\n",
    "You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA: {datum[\"sql_context\"]}\n",
    "{datum[\"sql_prompt\"]}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "{datum[\"sql\"]}<end_of_turn>\n",
    "\"\"\".strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9579d79-4c9f-4075-8942-9fee36601e94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"google/gemma-2-9b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    add_eos_token=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c098352b-80e4-4752-b643-9d02b19b5b31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_column = [generate_prompt(datum) for datum in dataset]\n",
    "dataset = dataset.add_column(\"prompt\", prompt_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cef6d0dc-2817-431c-b58b-d1e53c8e0704",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove columns: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'sql', 'sql_explanation']\n"
     ]
    }
   ],
   "source": [
    "columns_to_remove = list(dataset.features)\n",
    "for k in [\"prompt\"]:\n",
    "    columns_to_remove.remove(k)\n",
    "print(f\"remove columns: {columns_to_remove}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a0d1c61-d801-4318-b10e-f77334ec979c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.map(\n",
    "    lambda samples: tokenizer(samples[\"prompt\"]),\n",
    "    batched=True,\n",
    "    remove_columns=columns_to_remove,\n",
    ")\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4565381-4339-4063-880a-996cae26568f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][345][\"input_ids\"][-1] == tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c609924d-0a7a-4b94-8bd1-70ee3b48b2f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/fsspec/registry.py:279: UserWarning: Your installed version of s3fs is very old and known to cause\n",
      "severe performance issues, see also https://github.com/dask/dask/issues/10276\n",
      "\n",
      "To fix, you should specify a lower version bound on s3fs, or\n",
      "update the current installation.\n",
      "\n",
      "  warnings.warn(s3_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0856ce8a400405fa25a4c3f0d26e264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b421338836452daeb8a1fcd67b53ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data uploaded to:\n",
      "s3://sagemaker-us-east-1-395271362395/datasets/text-to-sql/train_dataset.json\n",
      "https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-395271362395/?region=us-east-1&prefix=datasets/text-to-sql/\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3 using our SageMaker session\n",
    "training_input_path = f\"s3://{sess.default_bucket()}/datasets/text-to-sql\"\n",
    "\n",
    "# save datasets to s3\n",
    "dataset[\"train\"].to_json(f\"{training_input_path}/train_dataset.json\", orient=\"records\")\n",
    "dataset[\"test\"].to_json(f\"{training_input_path}/test_dataset.json\", orient=\"records\")\n",
    "\n",
    "print(f\"Training data uploaded to:\")\n",
    "print(f\"{training_input_path}/train_dataset.json\")\n",
    "print(\n",
    "    f\"https://s3.console.aws.amazon.com/s3/buckets/{sess.default_bucket()}/?region={sess.boto_region_name}&prefix={training_input_path.split('/', 3)[-1]}/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f077492-56f3-482b-a250-68e270c2a2d4",
   "metadata": {},
   "source": [
    "# 3. Fine-Tune Gemma2 with QLoRA on Amazon Sagemaker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13ef4988-2c82-479c-9e8b-88284b2d0ed3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "from dataclasses import dataclass, field, fields, asdict\n",
    "from trl import SFTConfig\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PartialTrainingArguments:\n",
    "    num_train_epochs: int\n",
    "    per_device_train_batch_size: int\n",
    "    gradient_accumulation_steps: int\n",
    "    gradient_checkpointing: bool\n",
    "    optim: str\n",
    "    logging_steps: int\n",
    "    save_strategy: int\n",
    "    learning_rate: float\n",
    "    bf16: bool\n",
    "    tf32: bool\n",
    "    max_grad_norm: float\n",
    "    warmup_ratio: float\n",
    "    lr_scheduler_type: str\n",
    "    report_to: str\n",
    "    output_dir: str\n",
    "    fsdp: str\n",
    "    fsdp_config: Optional[Union[dict, str]]\n",
    "    # SFTrainer Config\n",
    "    dataset_text_field: str\n",
    "    packing: bool\n",
    "    max_seq_length: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Hyperparameters(PartialTrainingArguments):\n",
    "    # path where sagemaker will save training dataset\n",
    "    train_dataset_path: str\n",
    "    test_dataset_path: str\n",
    "    model_id: str\n",
    "\n",
    "    def to_dict(self):\n",
    "        return asdict(self)\n",
    "\n",
    "\n",
    "# Validate training arguments\n",
    "training_args_fields = {field.name for field in fields(SFTConfig)}\n",
    "partial_training_args_fields = {\n",
    "    field.name for field in fields(PartialTrainingArguments)\n",
    "}\n",
    "is_subset = partial_training_args_fields.issubset(training_args_fields)\n",
    "assert is_subset, \"All fields in PartialTrainingArguments should be in SFTConfig\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20facf95-1c8c-40fc-9388-2d841ed7f680",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_train_epochs': 1, 'per_device_train_batch_size': 12, 'gradient_accumulation_steps': 4, 'gradient_checkpointing': True, 'optim': 'paged_adamw_8bit', 'logging_steps': 10, 'save_strategy': 'epoch', 'learning_rate': 0.0002, 'bf16': True, 'tf32': True, 'max_grad_norm': 0.3, 'warmup_ratio': 0.03, 'lr_scheduler_type': 'constant', 'report_to': 'tensorboard', 'output_dir': '/tmp/tun', 'fsdp': 'shard_grad_op auto_wrap offload', 'fsdp_config': {'transformer_layer_cls_to_wrap': 'GemmaDecoderLayer', 'backward_prefetch': 'backward_pre', 'forward_prefetch': False, 'use_orig_params': False, 'cpu_ram_efficient_loading': True}, 'dataset_text_field': 'prompt', 'packing': True, 'max_seq_length': 1024, 'train_dataset_path': '/opt/ml/input/data/training/train_dataset.json', 'test_dataset_path': '/opt/ml/input/data/training/test_dataset.json', 'model_id': 'google/gemma-2-9b'}\n"
     ]
    }
   ],
   "source": [
    "from transformers.trainer_utils import FSDPOption\n",
    "\n",
    "hyperparameters = Hyperparameters(\n",
    "    ### SagemakerArguments ###\n",
    "    train_dataset_path=\"/opt/ml/input/data/training/train_dataset.json\",\n",
    "    test_dataset_path=\"/opt/ml/input/data/training/test_dataset.json\",\n",
    "    model_id=\"google/gemma-2-9b\",\n",
    "    ### TrainingArguments ###\n",
    "    num_train_epochs=3,  # number of training epochs\n",
    "    per_device_train_batch_size=4,  # batch size per device during training\n",
    "    gradient_accumulation_steps=4,  # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,  # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_8bit\",  # use adamw_8bit optimizer\n",
    "    logging_steps=10,  # log every 10 steps\n",
    "    save_strategy=\"epoch\",  # save checkpoint every epoch\n",
    "    learning_rate=2e-4,  # learning rate, based on QLoRA paper\n",
    "    bf16=True,  # use bfloat16 precision\n",
    "    tf32=True,  # use tf32 precision\n",
    "    max_grad_norm=0.3,  # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,  # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",  # use constant learning rate scheduler\n",
    "    report_to=\"tensorboard\",  # report metrics to tensorboard\n",
    "    output_dir=\"/tmp/tun\",  # Temporary output directory for model checkpoints\n",
    "    fsdp=f\"{FSDPOption.SHARD_GRAD_OP} {FSDPOption.AUTO_WRAP} {FSDPOption.OFFLOAD}\",\n",
    "    fsdp_config={\n",
    "        \"transformer_layer_cls_to_wrap\": \"GemmaDecoderLayer\",\n",
    "        \"backward_prefetch\": \"backward_pre\",\n",
    "        \"forward_prefetch\": False,\n",
    "        \"use_orig_params\": False,\n",
    "        \"cpu_ram_efficient_loading\": True,\n",
    "    },\n",
    "    ### SFTrainer Config ###\n",
    "    dataset_text_field=\"prompt\",\n",
    "    packing=True,\n",
    "    max_seq_length=512,\n",
    ").to_dict()\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc1d9dd3-abdd-4aeb-a70b-4859b4295c51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f\"gemma-9b-fsdp-text-to-sql\"\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point=\"run_sft_fsdp.py\",  # train script\n",
    "    source_dir=\"./scripts\",  # directory which includes all the files needed for training\n",
    "    instance_type=\"ml.p4d.24xlarge\",  # instances type used for the training job\n",
    "    instance_count=1,  # the number of instances used for training\n",
    "    max_run=2\n",
    "    * 24\n",
    "    * 3600,  # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name=job_name,  # the name of the training job\n",
    "    role=role,  # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size=300,  # the size of the EBS volume in GB\n",
    "    transformers_version=\"4.36\",  # the transformers version used in the training job\n",
    "    pytorch_version=\"2.1\",  # the pytorch_version version used in the training job\n",
    "    py_version=\"py310\",  # the python version used in the training job\n",
    "    hyperparameters=hyperparameters,  # the hyperparameters passed to the training job\n",
    "    disable_output_compression=True,  # not compress output to save training time and cost\n",
    "    environment={\n",
    "        \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\",  # set env variable to cache models in /tmp\n",
    "        \"HF_TOKEN\": \"REPLACE_WITH_YOUR_TOKEN\",  # huggingface token to access gated models, e.g. llama 2\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353ae3dd-75f1-4d65-96ea-836df2b0f001",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: gemma-9b-fsdp-text-to-sql-2024-07-20-08-36-34-388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-20 08:36:34 Starting - Starting the training job...\n",
      "2024-07-20 08:36:34 Pending - Training job waiting for capacity........................................................................."
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\"training\": training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddac057-fcf0-4749-a95f-da66813e7ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
