{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab8b4a5c-8198-486a-86ed-a6cf048ba8c4",
   "metadata": {},
   "source": [
    "# 1. Setup Development Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "529f42fd-c559-42ec-8646-6e5d0f10c532",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q -U trl\n",
    "!pip install -q -U sagemaker\n",
    "!pip install -q -U \"datasets[s3]\"\n",
    "!pip install -q -U \"huggingface_hub[cli]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1254a7c0-9fba-415d-b4e3-ca5d907786a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: datasets\n",
      "Version: 2.20.0\n",
      "Summary: HuggingFace community-driven open-source library of datasets\n",
      "Home-page: https://github.com/huggingface/datasets\n",
      "Author: HuggingFace Inc.\n",
      "Author-email: thomas@huggingface.co\n",
      "License: Apache 2.0\n",
      "Location: /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages\n",
      "Requires: aiohttp, dill, filelock, fsspec, huggingface-hub, multiprocess, numpy, packaging, pandas, pyarrow, pyarrow-hotfix, pyyaml, requests, tqdm, xxhash\n",
      "Required-by: trl\n",
      "Name: pandas\n",
      "Version: 1.5.3\n",
      "Summary: Powerful data structures for data analysis, time series, and statistics\n",
      "Home-page: https://pandas.pydata.org\n",
      "Author: The Pandas Development Team\n",
      "Author-email: pandas-dev@python.org\n",
      "License: BSD-3-Clause\n",
      "Location: /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages\n",
      "Requires: numpy, python-dateutil, pytz\n",
      "Required-by: autovizwidget, bokeh, datasets, hdijupyterutils, nvgpu, sagemaker, seaborn, shap, smclarify, sparkmagic, statsmodels\n"
     ]
    }
   ],
   "source": [
    "!pip show datasets\n",
    "!pip show pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fb90b63-d43d-43cb-816a-847a001c436e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12dcda282fa44aae980248fae89e963a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34eed579-7ba7-4f0a-a6e7-5aeadd64b8a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::395271362395:role/SagemakerStudioDemoSagema-SageMakerExecutionRole78-5I33I083KE6P\n",
      "sagemaker bucket: sagemaker-us-east-1-395271362395\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"AmazonSageMaker-ExecutionRole-20230112T181165\")[\n",
    "        \"Role\"\n",
    "    ][\"Arn\"]\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bf1360-9fd5-401f-9eac-81f6d0ebcac2",
   "metadata": {},
   "source": [
    "# 2. Create and prepare the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a45f0aa1-3427-446a-b9ac-225692cd6f3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'sql', 'sql_explanation'],\n",
       "    num_rows: 12500\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"gretelai/synthetic_text_to_sql\", split=\"train\")\n",
    "dataset = dataset.shuffle(seed=1234).select(range(12500))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "af4c82e2-992c-4db1-9088-5cc469dd4411",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>domain</th>\n",
       "      <th>domain_description</th>\n",
       "      <th>sql_complexity</th>\n",
       "      <th>sql_complexity_description</th>\n",
       "      <th>sql_task_type</th>\n",
       "      <th>sql_task_type_description</th>\n",
       "      <th>sql_prompt</th>\n",
       "      <th>sql_context</th>\n",
       "      <th>sql</th>\n",
       "      <th>sql_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65582</td>\n",
       "      <td>disability services</td>\n",
       "      <td>Comprehensive data on disability accommodation...</td>\n",
       "      <td>basic SQL</td>\n",
       "      <td>basic SQL with a simple select statement</td>\n",
       "      <td>data manipulation</td>\n",
       "      <td>inserting, updating, or deleting records</td>\n",
       "      <td>Update the budget for the 'ASL Interpreter' se...</td>\n",
       "      <td>CREATE TABLE Regions (RegionID INT, RegionName...</td>\n",
       "      <td>UPDATE SupportServices SET Budget = 16000 WHER...</td>\n",
       "      <td>This query updates the budget for the 'ASL Int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83180</td>\n",
       "      <td>climate change</td>\n",
       "      <td>Climate change data on climate mitigation, cli...</td>\n",
       "      <td>basic SQL</td>\n",
       "      <td>basic SQL with a simple select statement</td>\n",
       "      <td>analytics and reporting</td>\n",
       "      <td>generating reports, dashboards, and analytical...</td>\n",
       "      <td>Find the intersection of mitigation and adapta...</td>\n",
       "      <td>CREATE TABLE mitigation (id INT PRIMARY KEY, c...</td>\n",
       "      <td>SELECT m.action FROM mitigation m, adaptation ...</td>\n",
       "      <td>This SQL query identifies the intersection of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90518</td>\n",
       "      <td>marine biology</td>\n",
       "      <td>Comprehensive data on marine species, oceanogr...</td>\n",
       "      <td>basic SQL</td>\n",
       "      <td>basic SQL with a simple select statement</td>\n",
       "      <td>analytics and reporting</td>\n",
       "      <td>generating reports, dashboards, and analytical...</td>\n",
       "      <td>List all marine species with a conservation st...</td>\n",
       "      <td>CREATE TABLE species (id INT, name VARCHAR(255...</td>\n",
       "      <td>SELECT name FROM species WHERE conservation_st...</td>\n",
       "      <td>The SQL query filters the species table for ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42346</td>\n",
       "      <td>rural development</td>\n",
       "      <td>Agricultural innovation metrics, rural infrast...</td>\n",
       "      <td>basic SQL</td>\n",
       "      <td>basic SQL with a simple select statement</td>\n",
       "      <td>analytics and reporting</td>\n",
       "      <td>generating reports, dashboards, and analytical...</td>\n",
       "      <td>Find the minimum budget for agricultural innov...</td>\n",
       "      <td>CREATE TABLE agricultural_innovation (id INT, ...</td>\n",
       "      <td>SELECT MIN(budget) FROM agricultural_innovation;</td>\n",
       "      <td>The SQL query calculates the minimum budget fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86672</td>\n",
       "      <td>retail</td>\n",
       "      <td>Retail data on circular supply chains, ethical...</td>\n",
       "      <td>single join</td>\n",
       "      <td>only one join (specify inner, outer, cross)</td>\n",
       "      <td>analytics and reporting</td>\n",
       "      <td>generating reports, dashboards, and analytical...</td>\n",
       "      <td>What is the maximum price of a product sold by...</td>\n",
       "      <td>CREATE TABLE vendors(vendor_id INT, vendor_nam...</td>\n",
       "      <td>SELECT MAX(transactions.price) FROM transactio...</td>\n",
       "      <td>The SQL query calculates the maximum price of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>65425</td>\n",
       "      <td>arctic research</td>\n",
       "      <td>In-depth data on climate change, biodiversity,...</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>aggregation functions (COUNT, SUM, AVG, MIN, M...</td>\n",
       "      <td>analytics and reporting</td>\n",
       "      <td>generating reports, dashboards, and analytical...</td>\n",
       "      <td>What is the average temperature recorded in ea...</td>\n",
       "      <td>CREATE TABLE WeatherData (Station VARCHAR(255)...</td>\n",
       "      <td>SELECT Station, AVG(Temperature) FROM WeatherD...</td>\n",
       "      <td>This SQL query calculates the average temperat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>62717</td>\n",
       "      <td>arts and culture</td>\n",
       "      <td>Audience demographics, event attendance, progr...</td>\n",
       "      <td>subqueries</td>\n",
       "      <td>subqueries, including correlated and nested su...</td>\n",
       "      <td>data manipulation</td>\n",
       "      <td>inserting, updating, or deleting records</td>\n",
       "      <td>Delete records of artists who have not partici...</td>\n",
       "      <td>CREATE TABLE Artists (artist_id INT, artist_na...</td>\n",
       "      <td>DELETE FROM Artists WHERE artist_id NOT IN (SE...</td>\n",
       "      <td>This query deletes records of artists from the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10921</td>\n",
       "      <td>retail</td>\n",
       "      <td>Retail data on circular supply chains, ethical...</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>aggregation functions (COUNT, SUM, AVG, MIN, M...</td>\n",
       "      <td>analytics and reporting</td>\n",
       "      <td>generating reports, dashboards, and analytical...</td>\n",
       "      <td>List all suppliers that have provided both rec...</td>\n",
       "      <td>CREATE TABLE suppliers (supplier_id INT, suppl...</td>\n",
       "      <td>SELECT supplier_name FROM suppliers WHERE mate...</td>\n",
       "      <td>This query identifies all suppliers that have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12727</td>\n",
       "      <td>manufacturing</td>\n",
       "      <td>Detailed records on ethical manufacturing, cir...</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>aggregation functions (COUNT, SUM, AVG, MIN, M...</td>\n",
       "      <td>analytics and reporting</td>\n",
       "      <td>generating reports, dashboards, and analytical...</td>\n",
       "      <td>What is the total number of employees working ...</td>\n",
       "      <td>CREATE TABLE ethical_manufacturing (country VA...</td>\n",
       "      <td>SELECT country, SUM(employees) as total_employ...</td>\n",
       "      <td>This query calculates the total number of empl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>73714</td>\n",
       "      <td>sports entertainment</td>\n",
       "      <td>Sports team performance data, fan demographics...</td>\n",
       "      <td>single join</td>\n",
       "      <td>only one join (specify inner, outer, cross)</td>\n",
       "      <td>analytics and reporting</td>\n",
       "      <td>generating reports, dashboards, and analytical...</td>\n",
       "      <td>List the total number of tickets sold for home...</td>\n",
       "      <td>CREATE TABLE teams (team_id INT, team_name VAR...</td>\n",
       "      <td>SELECT t.team_name, SUM(g.price * g.attendance...</td>\n",
       "      <td>Join teams and games tables, filter on games f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                domain  \\\n",
       "0  65582   disability services   \n",
       "1  83180        climate change   \n",
       "2  90518        marine biology   \n",
       "3  42346     rural development   \n",
       "4  86672                retail   \n",
       "5  65425       arctic research   \n",
       "6  62717      arts and culture   \n",
       "7  10921                retail   \n",
       "8  12727         manufacturing   \n",
       "9  73714  sports entertainment   \n",
       "\n",
       "                                  domain_description sql_complexity  \\\n",
       "0  Comprehensive data on disability accommodation...      basic SQL   \n",
       "1  Climate change data on climate mitigation, cli...      basic SQL   \n",
       "2  Comprehensive data on marine species, oceanogr...      basic SQL   \n",
       "3  Agricultural innovation metrics, rural infrast...      basic SQL   \n",
       "4  Retail data on circular supply chains, ethical...    single join   \n",
       "5  In-depth data on climate change, biodiversity,...    aggregation   \n",
       "6  Audience demographics, event attendance, progr...     subqueries   \n",
       "7  Retail data on circular supply chains, ethical...    aggregation   \n",
       "8  Detailed records on ethical manufacturing, cir...    aggregation   \n",
       "9  Sports team performance data, fan demographics...    single join   \n",
       "\n",
       "                          sql_complexity_description            sql_task_type  \\\n",
       "0           basic SQL with a simple select statement        data manipulation   \n",
       "1           basic SQL with a simple select statement  analytics and reporting   \n",
       "2           basic SQL with a simple select statement  analytics and reporting   \n",
       "3           basic SQL with a simple select statement  analytics and reporting   \n",
       "4        only one join (specify inner, outer, cross)  analytics and reporting   \n",
       "5  aggregation functions (COUNT, SUM, AVG, MIN, M...  analytics and reporting   \n",
       "6  subqueries, including correlated and nested su...        data manipulation   \n",
       "7  aggregation functions (COUNT, SUM, AVG, MIN, M...  analytics and reporting   \n",
       "8  aggregation functions (COUNT, SUM, AVG, MIN, M...  analytics and reporting   \n",
       "9        only one join (specify inner, outer, cross)  analytics and reporting   \n",
       "\n",
       "                           sql_task_type_description  \\\n",
       "0           inserting, updating, or deleting records   \n",
       "1  generating reports, dashboards, and analytical...   \n",
       "2  generating reports, dashboards, and analytical...   \n",
       "3  generating reports, dashboards, and analytical...   \n",
       "4  generating reports, dashboards, and analytical...   \n",
       "5  generating reports, dashboards, and analytical...   \n",
       "6           inserting, updating, or deleting records   \n",
       "7  generating reports, dashboards, and analytical...   \n",
       "8  generating reports, dashboards, and analytical...   \n",
       "9  generating reports, dashboards, and analytical...   \n",
       "\n",
       "                                          sql_prompt  \\\n",
       "0  Update the budget for the 'ASL Interpreter' se...   \n",
       "1  Find the intersection of mitigation and adapta...   \n",
       "2  List all marine species with a conservation st...   \n",
       "3  Find the minimum budget for agricultural innov...   \n",
       "4  What is the maximum price of a product sold by...   \n",
       "5  What is the average temperature recorded in ea...   \n",
       "6  Delete records of artists who have not partici...   \n",
       "7  List all suppliers that have provided both rec...   \n",
       "8  What is the total number of employees working ...   \n",
       "9  List the total number of tickets sold for home...   \n",
       "\n",
       "                                         sql_context  \\\n",
       "0  CREATE TABLE Regions (RegionID INT, RegionName...   \n",
       "1  CREATE TABLE mitigation (id INT PRIMARY KEY, c...   \n",
       "2  CREATE TABLE species (id INT, name VARCHAR(255...   \n",
       "3  CREATE TABLE agricultural_innovation (id INT, ...   \n",
       "4  CREATE TABLE vendors(vendor_id INT, vendor_nam...   \n",
       "5  CREATE TABLE WeatherData (Station VARCHAR(255)...   \n",
       "6  CREATE TABLE Artists (artist_id INT, artist_na...   \n",
       "7  CREATE TABLE suppliers (supplier_id INT, suppl...   \n",
       "8  CREATE TABLE ethical_manufacturing (country VA...   \n",
       "9  CREATE TABLE teams (team_id INT, team_name VAR...   \n",
       "\n",
       "                                                 sql  \\\n",
       "0  UPDATE SupportServices SET Budget = 16000 WHER...   \n",
       "1  SELECT m.action FROM mitigation m, adaptation ...   \n",
       "2  SELECT name FROM species WHERE conservation_st...   \n",
       "3   SELECT MIN(budget) FROM agricultural_innovation;   \n",
       "4  SELECT MAX(transactions.price) FROM transactio...   \n",
       "5  SELECT Station, AVG(Temperature) FROM WeatherD...   \n",
       "6  DELETE FROM Artists WHERE artist_id NOT IN (SE...   \n",
       "7  SELECT supplier_name FROM suppliers WHERE mate...   \n",
       "8  SELECT country, SUM(employees) as total_employ...   \n",
       "9  SELECT t.team_name, SUM(g.price * g.attendance...   \n",
       "\n",
       "                                     sql_explanation  \n",
       "0  This query updates the budget for the 'ASL Int...  \n",
       "1  This SQL query identifies the intersection of ...  \n",
       "2  The SQL query filters the species table for ro...  \n",
       "3  The SQL query calculates the minimum budget fo...  \n",
       "4  The SQL query calculates the maximum price of ...  \n",
       "5  This SQL query calculates the average temperat...  \n",
       "6  This query deletes records of artists from the...  \n",
       "7  This query identifies all suppliers that have ...  \n",
       "8  This query calculates the total number of empl...  \n",
       "9  Join teams and games tables, filter on games f...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataset.to_pandas()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "52ab09f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_prompt(datum):\n",
    "    prompt = f\"\"\"\n",
    "<start_of_turn>user\n",
    "You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA: {datum[\"sql_context\"]}\n",
    "{datum[\"sql_prompt\"]}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "{datum[\"sql\"]}<end_of_turn>\n",
    "\"\"\".strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eb01e135-389a-44f1-9003-a2a676559457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"google/gemma-2-9b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    add_eos_token=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cef6d0dc-2817-431c-b58b-d1e53c8e0704",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_column = [generate_prompt(datum) for datum in dataset]\n",
    "dataset = dataset.add_column(\"prompt\", prompt_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "06558fb3-a865-4b65-ae52-d41ac29731ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove columns: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'sql', 'sql_explanation']\n"
     ]
    }
   ],
   "source": [
    "columns_to_remove = list(dataset.features)\n",
    "for k in [\"prompt\"]:\n",
    "    columns_to_remove.remove(k)\n",
    "print(f\"remove columns: {columns_to_remove}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8a0d1c61-d801-4318-b10e-f77334ec979c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e33263c167fb41ca98440c3d284f9450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.map(\n",
    "    lambda samples: tokenizer(samples[\"prompt\"]),\n",
    "    batched=True,\n",
    "    remove_columns=columns_to_remove,\n",
    ")\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c4565381-4339-4063-880a-996cae26568f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][345][\"input_ids\"][-1] == tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c609924d-0a7a-4b94-8bd1-70ee3b48b2f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672eb7b426a949e499f34d33bb3a070b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfa4d6a6e5ae4adb90a320044135813f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data uploaded to:\n",
      "s3://sagemaker-us-east-1-395271362395/datasets/text-to-sql/train_dataset.json\n",
      "https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-395271362395/?region=us-east-1&prefix=datasets/text-to-sql/\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3 using our SageMaker session\n",
    "training_input_path = f\"s3://{sess.default_bucket()}/datasets/text-to-sql\"\n",
    "\n",
    "# save datasets to s3\n",
    "dataset[\"train\"].to_json(f\"{training_input_path}/train_dataset.json\", orient=\"records\")\n",
    "dataset[\"test\"].to_json(f\"{training_input_path}/test_dataset.json\", orient=\"records\")\n",
    "\n",
    "print(f\"Training data uploaded to:\")\n",
    "print(f\"{training_input_path}/train_dataset.json\")\n",
    "print(\n",
    "    f\"https://s3.console.aws.amazon.com/s3/buckets/{sess.default_bucket()}/?region={sess.boto_region_name}&prefix={training_input_path.split('/', 3)[-1]}/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f077492-56f3-482b-a250-68e270c2a2d4",
   "metadata": {},
   "source": [
    "# 3. Fine-Tune Gemma2 with QLoRA on Amazon Sagemaker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "13ef4988-2c82-479c-9e8b-88284b2d0ed3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field, fields, asdict\n",
    "from trl import SFTConfig\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PartialTrainingArguments:\n",
    "    num_train_epochs: int\n",
    "    per_device_train_batch_size: int\n",
    "    gradient_accumulation_steps: int\n",
    "    gradient_checkpointing: bool\n",
    "    optim: str\n",
    "    logging_steps: int\n",
    "    save_strategy: int\n",
    "    learning_rate: float\n",
    "    bf16: bool\n",
    "    tf32: bool\n",
    "    max_grad_norm: float\n",
    "    warmup_ratio: float\n",
    "    lr_scheduler_type: str\n",
    "    report_to: str\n",
    "    output_dir: str\n",
    "    # SFTrainer Config\n",
    "    dataset_text_field: str\n",
    "    packing: bool\n",
    "    max_seq_length: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Hyperparameters(PartialTrainingArguments):\n",
    "    # path where sagemaker will save training dataset\n",
    "    train_dataset_path: str\n",
    "    test_dataset_path: str\n",
    "    model_id: str\n",
    "\n",
    "    def to_dict(self):\n",
    "        return asdict(self)\n",
    "\n",
    "\n",
    "# Validate training arguments\n",
    "training_args_fields = {field.name for field in fields(SFTConfig)}\n",
    "partial_training_args_fields = {\n",
    "    field.name for field in fields(PartialTrainingArguments)\n",
    "}\n",
    "is_subset = partial_training_args_fields.issubset(training_args_fields)\n",
    "assert is_subset, \"All fields in PartialTrainingArguments should be in SFTConfig\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "20facf95-1c8c-40fc-9388-2d841ed7f680",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_train_epochs': 3, 'per_device_train_batch_size': 1, 'gradient_accumulation_steps': 4, 'gradient_checkpointing': True, 'optim': 'adamw_8bit', 'logging_steps': 10, 'save_strategy': 'epoch', 'learning_rate': 0.0002, 'bf16': True, 'tf32': True, 'max_grad_norm': 0.3, 'warmup_ratio': 0.03, 'lr_scheduler_type': 'constant', 'report_to': 'tensorboard', 'output_dir': '/tmp/tun', 'dataset_text_field': 'prompt', 'packing': True, 'max_seq_length': 512, 'train_dataset_path': '/opt/ml/input/data/training/train_dataset.json', 'test_dataset_path': '/opt/ml/input/data/training/test_dataset.json', 'model_id': 'google/gemma-2-9b'}\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = Hyperparameters(\n",
    "    ### SagemakerArguments ###\n",
    "    train_dataset_path=\"/opt/ml/input/data/training/train_dataset.json\",\n",
    "    test_dataset_path=\"/opt/ml/input/data/training/test_dataset.json\",\n",
    "    model_id=\"google/gemma-2-9b\",\n",
    "    ### TrainingArguments ###\n",
    "    num_train_epochs=3,  # number of training epochs\n",
    "    per_device_train_batch_size=1,  # batch size per device during training\n",
    "    gradient_accumulation_steps=4,  # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,  # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_8bit\",  # use adamw_8bit optimizer\n",
    "    logging_steps=10,  # log every 10 steps\n",
    "    save_strategy=\"epoch\",  # save checkpoint every epoch\n",
    "    learning_rate=2e-4,  # learning rate, based on QLoRA paper\n",
    "    bf16=True,  # use bfloat16 precision\n",
    "    tf32=True,  # use tf32 precision\n",
    "    max_grad_norm=0.3,  # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,  # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",  # use constant learning rate scheduler\n",
    "    report_to=\"tensorboard\",  # report metrics to tensorboard\n",
    "    output_dir=\"/tmp/tun\",  # Temporary output directory for model checkpoints\n",
    "    ### SFTrainer Config ###\n",
    "    dataset_text_field=\"prompt\",\n",
    "    packing=True,\n",
    "    max_seq_length=512,\n",
    ").to_dict()\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dc1d9dd3-abdd-4aeb-a70b-4859b4295c51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f\"gemma-9b-text-to-sql\"\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point=\"run_sft.py\",  # train script\n",
    "    source_dir=\"./scripts\",  # directory which includes all the files needed for training\n",
    "    instance_type=\"ml.g5.16xlarge\",  # instances type used for the training job\n",
    "    instance_count=1,  # the number of instances used for training\n",
    "    max_run=2\n",
    "    * 24\n",
    "    * 3600,  # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name=job_name,  # the name of the training job\n",
    "    role=role,  # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size=300,  # the size of the EBS volume in GB\n",
    "    transformers_version=\"4.36\",  # the transformers version used in the training job\n",
    "    pytorch_version=\"2.1\",  # the pytorch_version version used in the training job\n",
    "    py_version=\"py310\",  # the python version used in the training job\n",
    "    hyperparameters=hyperparameters,  # the hyperparameters passed to the training job\n",
    "    disable_output_compression=True,  # not compress output to save training time and cost\n",
    "    environment={\n",
    "        \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\",  # set env variable to cache models in /tmp\n",
    "        \"HF_TOKEN\": HfFolder.get_token(),  # huggingface token to access gated models, e.g. gemma 2\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353ae3dd-75f1-4d65-96ea-836df2b0f001",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: gemma-9b-text-to-sql-2024-07-20-09-35-06-239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-20 09:35:06 Starting - Starting the training job...\n",
      "2024-07-20 09:35:23 Starting - Preparing the instances for training...\n",
      "2024-07-20 09:36:01 Downloading - Downloading input data...\n",
      "2024-07-20 09:36:16 Downloading - Downloading the training image..................\n",
      "2024-07-20 09:39:18 Training - Training image download completed. Training in progress....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-07-20 09:39:53,253 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-07-20 09:39:53,271 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-07-20 09:39:53,282 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-07-20 09:39:53,284 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-07-20 09:39:54,743 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers@ git+https://github.com/huggingface/transformers.git (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mCloning https://github.com/huggingface/transformers.git to /tmp/pip-install-l2v45vgj/transformers_c2f7be1dbc6743c38da18f198d3e8292\u001b[0m\n",
      "\u001b[34mRunning command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-install-l2v45vgj/transformers_c2f7be1dbc6743c38da18f198d3e8292\u001b[0m\n",
      "\u001b[34mResolved https://github.com/huggingface/transformers.git to commit 0fdea8607d7e01eb0e38a1ebeb7feee30a22f0cf\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting datasets==2.20.0 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.32.1 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.32.1-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting evaluate==0.4.2 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes==0.43.1 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting trl==0.9.6 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting peft==0.11.1 (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets==2.20.0->-r requirements.txt (line 2)) (3.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.20.0->-r requirements.txt (line 2)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.20.0->-r requirements.txt (line 2)) (15.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.20.0->-r requirements.txt (line 2)) (0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.20.0->-r requirements.txt (line 2)) (0.3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.20.0->-r requirements.txt (line 2)) (2.1.2)\u001b[0m\n",
      "\u001b[34mCollecting requests>=2.32.2 (from datasets==2.20.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting tqdm>=4.66.3 (from datasets==2.20.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.6/57.6 kB 9.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.20.0->-r requirements.txt (line 2)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.20.0->-r requirements.txt (line 2)) (0.70.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets==2.20.0->-r requirements.txt (line 2)) (2023.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.20.0->-r requirements.txt (line 2)) (3.9.3)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub>=0.21.2 (from datasets==2.20.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.24.0-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.20.0->-r requirements.txt (line 2)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.20.0->-r requirements.txt (line 2)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.32.1->-r requirements.txt (line 3)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.32.1->-r requirements.txt (line 3)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.32.1->-r requirements.txt (line 3)) (0.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl==0.9.6->-r requirements.txt (line 6)) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers@ git+https://github.com/huggingface/transformers.git->-r requirements.txt (line 1)) (2023.12.25)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.20,>=0.19 (from transformers@ git+https://github.com/huggingface/transformers.git->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 2)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 2)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 2)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 2)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 2)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets==2.20.0->-r requirements.txt (line 2)) (4.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets==2.20.0->-r requirements.txt (line 2)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets==2.20.0->-r requirements.txt (line 2)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets==2.20.0->-r requirements.txt (line 2)) (1.26.18)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets==2.20.0->-r requirements.txt (line 2)) (2023.7.22)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.32.1->-r requirements.txt (line 3)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.32.1->-r requirements.txt (line 3)) (3.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.32.1->-r requirements.txt (line 3)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.9.6->-r requirements.txt (line 6)) (0.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.9.6->-r requirements.txt (line 6)) (13.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.9.6->-r requirements.txt (line 6)) (1.6.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.20.0->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.20.0->-r requirements.txt (line 2)) (2023.3.post1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.20.0->-r requirements.txt (line 2)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.20.0->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.9.6->-r requirements.txt (line 6)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.9.6->-r requirements.txt (line 6)) (2.16.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.32.1->-r requirements.txt (line 3)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.32.1->-r requirements.txt (line 3)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.9.6->-r requirements.txt (line 6)) (0.1.0)\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.20.0-py3-none-any.whl (547 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 547.8/547.8 kB 50.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.32.1-py3-none-any.whl (314 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 314.1/314.1 kB 41.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.1/84.1 kB 13.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.8/119.8 MB 22.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading trl-0.9.6-py3-none-any.whl (245 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 245.8/245.8 kB 29.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading peft-0.11.1-py3-none-any.whl (251 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 251.6/251.6 kB 36.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.24.0-py3-none-any.whl (419 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 419.0/419.0 kB 45.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading requests-2.32.3-py3-none-any.whl (64 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.9/64.9 kB 11.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 90.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.3/78.3 kB 14.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: transformers\u001b[0m\n",
      "\u001b[34mBuilding wheel for transformers (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for transformers (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for transformers: filename=transformers-4.43.0.dev0-py3-none-any.whl size=9394962 sha256=1f28b35343fca9e9fa62d67c40701cf0e6d0730f2850cd2a81a189d832323689\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-yh0ey9xj/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\u001b[0m\n",
      "\u001b[34mSuccessfully built transformers\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tqdm, requests, huggingface-hub, tokenizers, bitsandbytes, accelerate, transformers, datasets, trl, peft, evaluate\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tqdm\u001b[0m\n",
      "\u001b[34mFound existing installation: tqdm 4.66.1\u001b[0m\n",
      "\u001b[34mUninstalling tqdm-4.66.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tqdm-4.66.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: requests\u001b[0m\n",
      "\u001b[34mFound existing installation: requests 2.31.0\u001b[0m\n",
      "\u001b[34mUninstalling requests-2.31.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled requests-2.31.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.20.3\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.20.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.20.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tokenizers\u001b[0m\n",
      "\u001b[34mFound existing installation: tokenizers 0.15.1\u001b[0m\n",
      "\u001b[34mUninstalling tokenizers-0.15.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tokenizers-0.15.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: bitsandbytes\u001b[0m\n",
      "\u001b[34mFound existing installation: bitsandbytes 0.42.0\u001b[0m\n",
      "\u001b[34mUninstalling bitsandbytes-0.42.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled bitsandbytes-0.42.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.25.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.25.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.25.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.36.0\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.36.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.36.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.15.0\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.15.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.15.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: trl\u001b[0m\n",
      "\u001b[34mFound existing installation: trl 0.7.4\u001b[0m\n",
      "\u001b[34mUninstalling trl-0.7.4:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled trl-0.7.4\u001b[0m\n",
      "\u001b[34mAttempting uninstall: peft\u001b[0m\n",
      "\u001b[34mFound existing installation: peft 0.7.1\u001b[0m\n",
      "\u001b[34mUninstalling peft-0.7.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled peft-0.7.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: evaluate\u001b[0m\n",
      "\u001b[34mFound existing installation: evaluate 0.4.1\u001b[0m\n",
      "\u001b[34mUninstalling evaluate-0.4.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled evaluate-0.4.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.32.1 bitsandbytes-0.43.1 datasets-2.20.0 evaluate-0.4.2 huggingface-hub-0.24.0 peft-0.11.1 requests-2.32.3 tokenizers-0.19.1 tqdm-4.66.4 transformers-4.43.0.dev0 trl-0.9.6\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.3.2 -> 24.1.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-07-20 09:40:24,046 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-07-20 09:40:24,046 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-07-20 09:40:24,089 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-07-20 09:40:24,120 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-07-20 09:40:24,151 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-07-20 09:40:24,164 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.16xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bf16\": true,\n",
      "        \"dataset_text_field\": \"prompt\",\n",
      "        \"gradient_accumulation_steps\": 4,\n",
      "        \"gradient_checkpointing\": true,\n",
      "        \"learning_rate\": 0.0002,\n",
      "        \"logging_steps\": 10,\n",
      "        \"lr_scheduler_type\": \"constant\",\n",
      "        \"max_grad_norm\": 0.3,\n",
      "        \"max_seq_length\": 512,\n",
      "        \"model_id\": \"google/gemma-2-9b\",\n",
      "        \"num_train_epochs\": 3,\n",
      "        \"optim\": \"adamw_8bit\",\n",
      "        \"output_dir\": \"/tmp/tun\",\n",
      "        \"packing\": true,\n",
      "        \"per_device_train_batch_size\": 1,\n",
      "        \"report_to\": \"tensorboard\",\n",
      "        \"save_strategy\": \"epoch\",\n",
      "        \"test_dataset_path\": \"/opt/ml/input/data/training/test_dataset.json\",\n",
      "        \"tf32\": true,\n",
      "        \"train_dataset_path\": \"/opt/ml/input/data/training/train_dataset.json\",\n",
      "        \"warmup_ratio\": 0.03\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.16xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": false,\n",
      "    \"job_name\": \"gemma-9b-text-to-sql-2024-07-20-09-35-06-239\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-395271362395/gemma-9b-text-to-sql-2024-07-20-09-35-06-239/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_sft\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.16xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.16xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_sft.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"bf16\":true,\"dataset_text_field\":\"prompt\",\"gradient_accumulation_steps\":4,\"gradient_checkpointing\":true,\"learning_rate\":0.0002,\"logging_steps\":10,\"lr_scheduler_type\":\"constant\",\"max_grad_norm\":0.3,\"max_seq_length\":512,\"model_id\":\"google/gemma-2-9b\",\"num_train_epochs\":3,\"optim\":\"adamw_8bit\",\"output_dir\":\"/tmp/tun\",\"packing\":true,\"per_device_train_batch_size\":1,\"report_to\":\"tensorboard\",\"save_strategy\":\"epoch\",\"test_dataset_path\":\"/opt/ml/input/data/training/test_dataset.json\",\"tf32\":true,\"train_dataset_path\":\"/opt/ml/input/data/training/train_dataset.json\",\"warmup_ratio\":0.03}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_sft.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.16xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.16xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.16xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.16xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_sft\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-395271362395/gemma-9b-text-to-sql-2024-07-20-09-35-06-239/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.16xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"bf16\":true,\"dataset_text_field\":\"prompt\",\"gradient_accumulation_steps\":4,\"gradient_checkpointing\":true,\"learning_rate\":0.0002,\"logging_steps\":10,\"lr_scheduler_type\":\"constant\",\"max_grad_norm\":0.3,\"max_seq_length\":512,\"model_id\":\"google/gemma-2-9b\",\"num_train_epochs\":3,\"optim\":\"adamw_8bit\",\"output_dir\":\"/tmp/tun\",\"packing\":true,\"per_device_train_batch_size\":1,\"report_to\":\"tensorboard\",\"save_strategy\":\"epoch\",\"test_dataset_path\":\"/opt/ml/input/data/training/test_dataset.json\",\"tf32\":true,\"train_dataset_path\":\"/opt/ml/input/data/training/train_dataset.json\",\"warmup_ratio\":0.03},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.16xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"gemma-9b-text-to-sql-2024-07-20-09-35-06-239\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-395271362395/gemma-9b-text-to-sql-2024-07-20-09-35-06-239/source/sourcedir.tar.gz\",\"module_name\":\"run_sft\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.16xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.16xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_sft.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--bf16\",\"True\",\"--dataset_text_field\",\"prompt\",\"--gradient_accumulation_steps\",\"4\",\"--gradient_checkpointing\",\"True\",\"--learning_rate\",\"0.0002\",\"--logging_steps\",\"10\",\"--lr_scheduler_type\",\"constant\",\"--max_grad_norm\",\"0.3\",\"--max_seq_length\",\"512\",\"--model_id\",\"google/gemma-2-9b\",\"--num_train_epochs\",\"3\",\"--optim\",\"adamw_8bit\",\"--output_dir\",\"/tmp/tun\",\"--packing\",\"True\",\"--per_device_train_batch_size\",\"1\",\"--report_to\",\"tensorboard\",\"--save_strategy\",\"epoch\",\"--test_dataset_path\",\"/opt/ml/input/data/training/test_dataset.json\",\"--tf32\",\"True\",\"--train_dataset_path\",\"/opt/ml/input/data/training/train_dataset.json\",\"--warmup_ratio\",\"0.03\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BF16=true\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_TEXT_FIELD=prompt\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=4\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_CHECKPOINTING=true\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0002\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=10\u001b[0m\n",
      "\u001b[34mSM_HP_LR_SCHEDULER_TYPE=constant\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_GRAD_NORM=0.3\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SEQ_LENGTH=512\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=google/gemma-2-9b\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=3\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIM=adamw_8bit\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/tmp/tun\u001b[0m\n",
      "\u001b[34mSM_HP_PACKING=true\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_REPORT_TO=tensorboard\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_TEST_DATASET_PATH=/opt/ml/input/data/training/test_dataset.json\u001b[0m\n",
      "\u001b[34mSM_HP_TF32=true\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_DATASET_PATH=/opt/ml/input/data/training/train_dataset.json\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_RATIO=0.03\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 run_sft.py --bf16 True --dataset_text_field prompt --gradient_accumulation_steps 4 --gradient_checkpointing True --learning_rate 0.0002 --logging_steps 10 --lr_scheduler_type constant --max_grad_norm 0.3 --max_seq_length 512 --model_id google/gemma-2-9b --num_train_epochs 3 --optim adamw_8bit --output_dir /tmp/tun --packing True --per_device_train_batch_size 1 --report_to tensorboard --save_strategy epoch --test_dataset_path /opt/ml/input/data/training/test_dataset.json --tf32 True --train_dataset_path /opt/ml/input/data/training/train_dataset.json --warmup_ratio 0.03\u001b[0m\n",
      "\u001b[34m2024-07-20 09:40:24,165 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2024-07-20 09:40:24,166 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 10000 examples [00:00, 77498.21 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 10000 examples [00:00, 77340.45 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 2500 examples [00:00, 122551.60 examples/s]\u001b[0m\n",
      "\u001b[34msample training prompt:  <start_of_turn>user\u001b[0m\n",
      "\u001b[34mYou are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\u001b[0m\n",
      "\u001b[34mSCHEMA: CREATE TABLE incidents (id INT, type VARCHAR(255), year INT, count INT);\u001b[0m\n",
      "\u001b[34mFind the total number of crimes and accidents for each type in a given year.<end_of_turn>\u001b[0m\n",
      "\u001b[34m<start_of_turn>model\u001b[0m\n",
      "\u001b[34mSELECT type, SUM(count) FROM incidents WHERE year = 2022 GROUP BY type;<end_of_turn>\u001b[0m\n",
      "\u001b[34msample training prompt:  <start_of_turn>user\u001b[0m\n",
      "\u001b[34mYou are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\u001b[0m\n",
      "\u001b[34mSCHEMA: CREATE TABLE IF NOT EXISTS public.bridges (id SERIAL PRIMARY KEY, name TEXT, design_load_rating REAL); INSERT INTO public.bridges (name, design_load_rating) SELECT 'Bridge001', 450.0 FROM generate_series(1, 10); INSERT INTO public.bridges (name, design_load_rating) SELECT 'Bridge002', 300.0 FROM generate_series(1, 10);\u001b[0m\n",
      "\u001b[34mWhat are the top 5 bridges with the highest design load ratings in the \"bridges\" table, ordered by design load rating in descending order?<end_of_turn>\u001b[0m\n",
      "\u001b[34m<start_of_turn>model\u001b[0m\n",
      "\u001b[34mSELECT name, design_load_rating FROM public.bridges ORDER BY design_load_rating DESC LIMIT 5;<end_of_turn>\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  12%|█▎        | 1/8 [00:14<01:44, 14.91s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 2/8 [00:30<01:32, 15.46s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  38%|███▊      | 3/8 [00:47<01:20, 16.19s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 4/8 [01:00<00:59, 14.85s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  62%|██████▎   | 5/8 [01:15<00:44, 14.81s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 6/8 [01:32<00:31, 15.72s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  88%|████████▊ | 7/8 [01:49<00:16, 16.08s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [01:55<00:00, 12.77s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 8/8 [01:55<00:00, 14.42s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:06,  1.14it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 2/8 [00:01<00:05,  1.13it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  38%|███▊      | 3/8 [00:02<00:04,  1.13it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 4/8 [00:03<00:03,  1.13it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  62%|██████▎   | 5/8 [00:04<00:02,  1.13it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 6/8 [00:05<00:01,  1.13it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 7/8 [00:06<00:00,  1.12it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:06<00:00,  1.32it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 8/8 [00:06<00:00,  1.20it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, packing. Will not be supported from version '1.0.0'.\u001b[0m\n",
      "\u001b[34mDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:192: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mtrainable params: 216,072,192 || all params: 9,457,778,176 || trainable%: 2.2846\u001b[0m\n",
      "\u001b[34m0%|          | 0/7500 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 1/7500 [00:05<10:49:55,  5.20s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/7500 [00:09<9:45:48,  4.69s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 3/7500 [00:14<9:33:32,  4.59s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 4/7500 [00:18<9:41:44,  4.66s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 5/7500 [00:22<9:10:43,  4.41s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 6/7500 [00:27<9:27:21,  4.54s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 7/7500 [00:31<9:15:20,  4.45s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 8/7500 [00:36<9:24:31,  4.52s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 9/7500 [00:40<9:08:51,  4.40s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 10/7500 [00:44<9:06:36,  4.38s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9802, 'grad_norm': 0.7066062092781067, 'learning_rate': 0.0002, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 10/7500 [00:44<9:06:36,  4.38s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 11/7500 [00:49<9:08:43,  4.40s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 12/7500 [00:54<9:18:07,  4.47s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 13/7500 [00:58<9:04:10,  4.36s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 14/7500 [01:02<9:13:16,  4.43s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 15/7500 [01:06<8:54:14,  4.28s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 16/7500 [01:11<9:05:14,  4.37s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 17/7500 [01:15<9:18:59,  4.48s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 18/7500 [01:20<9:19:09,  4.48s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 19/7500 [01:24<9:06:45,  4.39s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 20/7500 [01:28<8:52:18,  4.27s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6153, 'grad_norm': 0.6389077305793762, 'learning_rate': 0.0002, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m0%|          | 20/7500 [01:28<8:52:18,  4.27s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 21/7500 [01:32<8:46:32,  4.22s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 22/7500 [01:36<8:43:54,  4.20s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 23/7500 [01:41<8:41:52,  4.19s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 24/7500 [01:45<8:48:37,  4.24s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 25/7500 [01:49<9:00:33,  4.34s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 26/7500 [01:54<9:00:28,  4.34s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 27/7500 [01:58<8:55:49,  4.30s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 28/7500 [02:02<8:50:20,  4.26s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 29/7500 [02:06<8:48:26,  4.24s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 30/7500 [02:11<9:05:41,  4.38s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.5712, 'grad_norm': 0.4367581903934479, 'learning_rate': 0.0002, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m0%|          | 30/7500 [02:11<9:05:41,  4.38s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 31/7500 [02:15<8:51:17,  4.27s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 32/7500 [02:19<8:54:40,  4.30s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 33/7500 [02:24<8:51:31,  4.27s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 34/7500 [02:28<8:58:56,  4.33s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 35/7500 [02:33<9:14:20,  4.46s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 36/7500 [02:37<9:05:52,  4.39s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 37/7500 [02:41<9:01:54,  4.36s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 38/7500 [02:45<8:51:27,  4.27s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 39/7500 [02:50<9:02:32,  4.36s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 40/7500 [02:54<8:55:24,  4.31s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.5538, 'grad_norm': 0.4116416871547699, 'learning_rate': 0.0002, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m1%|          | 40/7500 [02:54<8:55:24,  4.31s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 41/7500 [02:58<8:43:23,  4.21s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 42/7500 [03:03<8:57:11,  4.32s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 43/7500 [03:07<8:51:06,  4.27s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 44/7500 [03:11<8:41:19,  4.20s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 45/7500 [03:15<8:43:22,  4.21s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 46/7500 [03:20<8:59:03,  4.34s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 47/7500 [03:24<8:50:30,  4.27s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 48/7500 [03:28<8:52:01,  4.28s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 49/7500 [03:32<8:41:02,  4.20s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 50/7500 [03:36<8:28:56,  4.10s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.5755, 'grad_norm': 0.46253904700279236, 'learning_rate': 0.0002, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m1%|          | 50/7500 [03:36<8:28:56,  4.10s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 51/7500 [03:40<8:37:44,  4.17s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 52/7500 [03:45<9:05:16,  4.39s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 53/7500 [03:50<9:15:53,  4.48s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 54/7500 [03:55<9:27:08,  4.57s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 55/7500 [03:59<9:06:47,  4.41s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 56/7500 [04:03<9:10:05,  4.43s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 57/7500 [04:08<9:04:14,  4.39s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 58/7500 [04:12<9:07:11,  4.41s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 59/7500 [04:17<9:10:02,  4.44s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 60/7500 [04:22<9:32:31,  4.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.5484, 'grad_norm': 0.31305065751075745, 'learning_rate': 0.0002, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m1%|          | 60/7500 [04:22<9:32:31,  4.62s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 61/7500 [04:26<9:26:18,  4.57s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 62/7500 [04:31<9:22:26,  4.54s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 63/7500 [04:35<9:00:18,  4.36s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 64/7500 [04:39<8:52:32,  4.30s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 65/7500 [04:43<9:03:24,  4.39s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 66/7500 [04:48<9:19:06,  4.51s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 67/7500 [04:52<9:03:32,  4.39s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 68/7500 [04:56<8:49:27,  4.27s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 69/7500 [05:01<9:09:15,  4.43s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 70/7500 [05:05<8:52:35,  4.30s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.5353, 'grad_norm': 0.3599231243133545, 'learning_rate': 0.0002, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m1%|          | 70/7500 [05:05<8:52:35,  4.30s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 71/7500 [05:09<8:43:32,  4.23s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 72/7500 [05:13<8:43:44,  4.23s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 73/7500 [05:18<8:45:01,  4.24s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 74/7500 [05:22<8:45:42,  4.25s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 75/7500 [05:26<8:38:41,  4.19s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 76/7500 [05:30<8:52:25,  4.30s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 77/7500 [05:34<8:42:00,  4.22s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 78/7500 [05:40<9:21:08,  4.54s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 79/7500 [05:45<9:31:54,  4.62s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 80/7500 [05:49<9:24:04,  4.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.5289, 'grad_norm': 0.37525635957717896, 'learning_rate': 0.0002, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m1%|          | 80/7500 [05:49<9:24:04,  4.56s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 81/7500 [05:53<9:11:12,  4.46s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 82/7500 [05:58<9:19:18,  4.52s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 83/7500 [06:02<9:19:26,  4.53s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 84/7500 [06:07<9:27:26,  4.59s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 85/7500 [06:11<9:11:39,  4.46s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 86/7500 [06:16<9:04:40,  4.41s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 87/7500 [06:20<9:00:30,  4.37s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 88/7500 [06:24<8:44:50,  4.25s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 89/7500 [06:28<8:56:31,  4.34s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 90/7500 [06:33<8:58:11,  4.36s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.5197, 'grad_norm': 0.3225553631782532, 'learning_rate': 0.0002, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m1%|          | 90/7500 [06:33<8:58:11,  4.36s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 91/7500 [06:37<8:45:35,  4.26s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 92/7500 [06:41<8:49:12,  4.29s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 93/7500 [06:45<8:44:44,  4.25s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 94/7500 [06:50<8:43:54,  4.24s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 95/7500 [06:54<8:50:36,  4.30s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 96/7500 [06:59<9:29:58,  4.62s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 97/7500 [07:04<9:15:03,  4.50s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 98/7500 [07:08<8:59:09,  4.37s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 99/7500 [07:12<9:06:46,  4.43s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 100/7500 [07:17<9:01:58,  4.39s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.5248, 'grad_norm': 0.4197719991207123, 'learning_rate': 0.0002, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m1%|▏         | 100/7500 [07:17<9:01:58,  4.39s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 101/7500 [07:21<9:13:22,  4.49s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 102/7500 [07:25<8:55:48,  4.35s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 103/7500 [07:29<8:44:37,  4.26s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 104/7500 [07:34<9:16:32,  4.51s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 105/7500 [07:39<9:17:19,  4.52s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 106/7500 [07:43<9:17:37,  4.53s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 107/7500 [07:48<9:01:52,  4.40s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 108/7500 [07:52<9:09:56,  4.46s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 109/7500 [07:57<9:07:29,  4.44s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 110/7500 [08:01<8:56:42,  4.36s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.5289, 'grad_norm': 0.3580474555492401, 'learning_rate': 0.0002, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m1%|▏         | 110/7500 [08:01<8:56:42,  4.36s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 111/7500 [08:05<8:39:59,  4.22s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 112/7500 [08:09<8:32:03,  4.16s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 113/7500 [08:13<8:25:46,  4.11s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 114/7500 [08:17<8:43:16,  4.25s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 115/7500 [08:22<8:55:04,  4.35s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 116/7500 [08:26<8:55:54,  4.35s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 117/7500 [08:30<8:38:06,  4.21s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 118/7500 [08:35<8:55:06,  4.35s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 119/7500 [08:39<9:09:13,  4.46s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 120/7500 [08:44<9:01:51,  4.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.5401, 'grad_norm': 0.32227623462677, 'learning_rate': 0.0002, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m2%|▏         | 120/7500 [08:44<9:01:51,  4.41s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 121/7500 [08:48<8:57:10,  4.37s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 122/7500 [08:53<9:17:45,  4.54s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 123/7500 [08:57<8:55:13,  4.35s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 124/7500 [09:01<9:02:25,  4.41s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 125/7500 [09:06<9:13:26,  4.50s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 126/7500 [09:10<9:01:27,  4.41s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 127/7500 [09:14<8:51:47,  4.33s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 128/7500 [09:19<8:55:56,  4.36s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 129/7500 [09:23<8:46:04,  4.28s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 130/7500 [09:28<9:00:53,  4.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.5312, 'grad_norm': 0.3722487688064575, 'learning_rate': 0.0002, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m2%|▏         | 130/7500 [09:28<9:00:53,  4.40s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 131/7500 [09:32<8:59:34,  4.39s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 132/7500 [09:36<8:55:56,  4.36s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 133/7500 [09:40<8:41:55,  4.25s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 134/7500 [09:44<8:38:35,  4.22s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 135/7500 [09:49<8:41:40,  4.25s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 136/7500 [09:53<8:46:30,  4.29s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 137/7500 [09:58<8:57:01,  4.38s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 138/7500 [10:02<8:56:31,  4.37s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 139/7500 [10:06<8:53:38,  4.35s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 140/7500 [10:11<9:15:53,  4.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.5196, 'grad_norm': 0.3715667426586151, 'learning_rate': 0.0002, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m2%|▏         | 140/7500 [10:11<9:15:53,  4.53s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 141/7500 [10:15<8:58:18,  4.39s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 142/7500 [10:20<9:05:15,  4.45s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 143/7500 [10:24<8:59:27,  4.40s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 144/7500 [10:29<9:10:19,  4.49s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 145/7500 [10:34<9:22:03,  4.59s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 146/7500 [10:38<9:08:47,  4.48s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 147/7500 [10:42<8:48:22,  4.31s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 148/7500 [10:46<8:38:31,  4.23s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 149/7500 [10:50<8:30:23,  4.17s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 150/7500 [10:54<8:27:23,  4.14s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.5399, 'grad_norm': 0.463886559009552, 'learning_rate': 0.0002, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m2%|▏         | 150/7500 [10:54<8:27:23,  4.14s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 151/7500 [10:58<8:27:12,  4.14s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 152/7500 [11:02<8:27:34,  4.14s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 153/7500 [11:06<8:22:18,  4.10s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 154/7500 [11:11<8:52:31,  4.35s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 155/7500 [11:16<9:08:45,  4.48s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 156/7500 [11:21<9:18:37,  4.56s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 157/7500 [11:25<9:11:36,  4.51s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 158/7500 [11:29<8:51:09,  4.34s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 159/7500 [11:34<8:56:29,  4.38s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 160/7500 [11:38<9:00:44,  4.42s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.5353, 'grad_norm': 0.3729373514652252, 'learning_rate': 0.0002, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m2%|▏         | 160/7500 [11:38<9:00:44,  4.42s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 161/7500 [11:43<9:13:56,  4.53s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 162/7500 [11:48<9:17:44,  4.56s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 163/7500 [11:51<8:50:46,  4.34s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 164/7500 [11:56<9:03:58,  4.45s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 165/7500 [12:01<9:08:09,  4.48s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 166/7500 [12:05<9:07:24,  4.48s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 167/7500 [12:10<9:11:00,  4.51s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 168/7500 [12:14<9:13:48,  4.53s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 169/7500 [12:19<9:15:46,  4.55s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 170/7500 [12:24<9:18:45,  4.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.5396, 'grad_norm': 0.3199199140071869, 'learning_rate': 0.0002, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m2%|▏         | 170/7500 [12:24<9:18:45,  4.57s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 171/7500 [12:28<8:55:31,  4.38s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 172/7500 [12:32<8:47:32,  4.32s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 173/7500 [12:36<8:41:44,  4.27s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 174/7500 [12:40<8:45:25,  4.30s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 175/7500 [12:45<9:07:31,  4.48s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 176/7500 [12:49<9:02:11,  4.44s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 177/7500 [12:54<8:57:06,  4.40s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 178/7500 [12:58<8:49:56,  4.34s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 179/7500 [13:02<8:34:31,  4.22s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 180/7500 [13:06<8:36:07,  4.23s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.53, 'grad_norm': 0.32216402888298035, 'learning_rate': 0.0002, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m2%|▏         | 180/7500 [13:06<8:36:07,  4.23s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 181/7500 [13:10<8:24:08,  4.13s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 182/7500 [13:15<8:39:16,  4.26s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 183/7500 [13:19<8:57:10,  4.40s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 184/7500 [13:24<9:20:21,  4.60s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 185/7500 [13:29<9:18:27,  4.58s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 186/7500 [13:33<9:16:17,  4.56s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 187/7500 [13:39<9:37:59,  4.74s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 188/7500 [13:43<9:09:07,  4.51s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 189/7500 [13:47<9:20:59,  4.60s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 190/7500 [13:52<9:27:51,  4.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.5055, 'grad_norm': 0.35905924439430237, 'learning_rate': 0.0002, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m3%|▎         | 190/7500 [13:52<9:27:51,  4.66s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 420/7500 [30:41<8:07:58,  4.14s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.5658, 'grad_norm': 0.42478030920028687, 'learning_rate': 0.0002, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m6%|▌         | 420/7500 [30:41<8:07:58,  4.14s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 421/7500 [30:45<8:20:15,  4.24s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 422/7500 [30:49<8:14:21,  4.19s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 423/7500 [30:54<8:26:43,  4.30s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 424/7500 [30:58<8:33:26,  4.35s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 648/7500 [47:18<8:26:54,  4.44s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 796/7500 [58:02<8:07:56,  4.37s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 797/7500 [58:07<8:13:04,  4.41s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 835/7500 [1:00:55<8:05:03,  4.37s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 836/7500 [1:00:59<7:57:05,  4.30s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 837/7500 [1:01:03<7:45:16,  4.19s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 838/7500 [1:01:07<7:46:40,  4.20s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 839/7500 [1:01:11<7:37:17,  4.12s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 840/7500 [1:01:16<8:03:30,  4.36s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4936, 'grad_norm': 0.2739483118057251, 'learning_rate': 0.0002, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m11%|█         | 840/7500 [1:01:16<8:03:30,  4.36s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 841/7500 [1:01:20<8:01:32,  4.34s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 842/7500 [1:01:25<8:12:20,  4.44s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 843/7500 [1:01:29<8:04:08,  4.36s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 844/7500 [1:01:33<8:00:35,  4.33s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 845/7500 [1:01:38<8:02:39,  4.35s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 846/7500 [1:01:42<8:01:40,  4.34s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 847/7500 [1:01:46<7:50:56,  4.25s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 848/7500 [1:01:51<7:59:13,  4.32s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 849/7500 [1:01:55<7:56:16,  4.30s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 850/7500 [1:02:00<8:22:11,  4.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4835, 'grad_norm': 0.26766127347946167, 'learning_rate': 0.0002, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m11%|█▏        | 850/7500 [1:02:00<8:22:11,  4.53s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 851/7500 [1:02:04<8:07:02,  4.39s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 852/7500 [1:02:08<8:02:33,  4.36s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 853/7500 [1:02:13<8:05:58,  4.39s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 854/7500 [1:02:18<8:30:37,  4.61s/it]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\"training\": training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddac057-fcf0-4749-a95f-da66813e7ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
