{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab8b4a5c-8198-486a-86ed-a6cf048ba8c4",
   "metadata": {},
   "source": [
    "# 1. Setup Development Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "529f42fd-c559-42ec-8646-6e5d0f10c532",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q -U trl\n",
    "!pip install -q -U sagemaker\n",
    "!pip install -q -U \"datasets[s3]\"\n",
    "!pip install -q -U \"huggingface_hub[cli]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1254a7c0-9fba-415d-b4e3-ca5d907786a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: datasets\n",
      "Version: 2.20.0\n",
      "Summary: HuggingFace community-driven open-source library of datasets\n",
      "Home-page: https://github.com/huggingface/datasets\n",
      "Author: HuggingFace Inc.\n",
      "Author-email: thomas@huggingface.co\n",
      "License: Apache 2.0\n",
      "Location: /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages\n",
      "Requires: aiohttp, dill, filelock, fsspec, huggingface-hub, multiprocess, numpy, packaging, pandas, pyarrow, pyarrow-hotfix, pyyaml, requests, tqdm, xxhash\n",
      "Required-by: trl\n",
      "Name: pandas\n",
      "Version: 1.5.3\n",
      "Summary: Powerful data structures for data analysis, time series, and statistics\n",
      "Home-page: https://pandas.pydata.org\n",
      "Author: The Pandas Development Team\n",
      "Author-email: pandas-dev@python.org\n",
      "License: BSD-3-Clause\n",
      "Location: /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages\n",
      "Requires: numpy, python-dateutil, pytz\n",
      "Required-by: autovizwidget, bokeh, datasets, hdijupyterutils, nvgpu, sagemaker, seaborn, shap, smclarify, sparkmagic, statsmodels\n"
     ]
    }
   ],
   "source": [
    "!pip show datasets\n",
    "!pip show pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae759202-910e-41f3-b825-6b587e7b8e18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !huggingface-cli login --token YOUR_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fb90b63-d43d-43cb-816a-847a001c436e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a7e72f50b234f7fa8e13e0f1771b4b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34eed579-7ba7-4f0a-a6e7-5aeadd64b8a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::395271362395:role/SagemakerStudioDemoSagema-SageMakerExecutionRole78-5I33I083KE6P\n",
      "sagemaker bucket: sagemaker-us-east-1-395271362395\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"AmazonSageMaker-ExecutionRole-20230112T181165\")[\n",
    "        \"Role\"\n",
    "    ][\"Arn\"]\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bf1360-9fd5-401f-9eac-81f6d0ebcac2",
   "metadata": {},
   "source": [
    "# 2. Create and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a45f0aa1-3427-446a-b9ac-225692cd6f3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'sql', 'sql_explanation'],\n",
       "    num_rows: 100000\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"gretelai/synthetic_text_to_sql\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af4c82e2-992c-4db1-9088-5cc469dd4411",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>domain</th>\n",
       "      <th>domain_description</th>\n",
       "      <th>sql_complexity</th>\n",
       "      <th>sql_complexity_description</th>\n",
       "      <th>sql_task_type</th>\n",
       "      <th>sql_task_type_description</th>\n",
       "      <th>sql_prompt</th>\n",
       "      <th>sql_context</th>\n",
       "      <th>sql</th>\n",
       "      <th>sql_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5097</td>\n",
       "      <td>forestry</td>\n",
       "      <td>Comprehensive data on sustainable forest manag...</td>\n",
       "      <td>single join</td>\n",
       "      <td>only one join (specify inner, outer, cross)</td>\n",
       "      <td>analytics and reporting</td>\n",
       "      <td>generating reports, dashboards, and analytical...</td>\n",
       "      <td>What is the total volume of timber sold by eac...</td>\n",
       "      <td>CREATE TABLE salesperson (salesperson_id INT, ...</td>\n",
       "      <td>SELECT salesperson_id, name, SUM(volume) as to...</td>\n",
       "      <td>Joins timber_sales and salesperson tables, gro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5098</td>\n",
       "      <td>defense industry</td>\n",
       "      <td>Defense contract data, military equipment main...</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>aggregation functions (COUNT, SUM, AVG, MIN, M...</td>\n",
       "      <td>analytics and reporting</td>\n",
       "      <td>generating reports, dashboards, and analytical...</td>\n",
       "      <td>List all the unique equipment types and their ...</td>\n",
       "      <td>CREATE TABLE equipment_maintenance (equipment_...</td>\n",
       "      <td>SELECT equipment_type, SUM(maintenance_frequen...</td>\n",
       "      <td>This query groups the equipment_maintenance ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5099</td>\n",
       "      <td>marine biology</td>\n",
       "      <td>Comprehensive data on marine species, oceanogr...</td>\n",
       "      <td>basic SQL</td>\n",
       "      <td>basic SQL with a simple select statement</td>\n",
       "      <td>analytics and reporting</td>\n",
       "      <td>generating reports, dashboards, and analytical...</td>\n",
       "      <td>How many marine species are found in the South...</td>\n",
       "      <td>CREATE TABLE marine_species (name VARCHAR(50),...</td>\n",
       "      <td>SELECT COUNT(*) FROM marine_species WHERE loca...</td>\n",
       "      <td>This query counts the number of marine species...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5100</td>\n",
       "      <td>financial services</td>\n",
       "      <td>Detailed financial data including investment s...</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>aggregation functions (COUNT, SUM, AVG, MIN, M...</td>\n",
       "      <td>analytics and reporting</td>\n",
       "      <td>generating reports, dashboards, and analytical...</td>\n",
       "      <td>What is the total trade value and average pric...</td>\n",
       "      <td>CREATE TABLE trade_history (id INT, trader_id ...</td>\n",
       "      <td>SELECT trader_id, stock, SUM(price * quantity)...</td>\n",
       "      <td>This query calculates the total trade value an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5101</td>\n",
       "      <td>energy</td>\n",
       "      <td>Energy market data covering renewable energy s...</td>\n",
       "      <td>window functions</td>\n",
       "      <td>window functions (e.g., ROW_NUMBER, LEAD, LAG,...</td>\n",
       "      <td>analytics and reporting</td>\n",
       "      <td>generating reports, dashboards, and analytical...</td>\n",
       "      <td>Find the energy efficiency upgrades with the h...</td>\n",
       "      <td>CREATE TABLE upgrades (id INT, cost FLOAT, typ...</td>\n",
       "      <td>SELECT type, cost FROM (SELECT type, cost, ROW...</td>\n",
       "      <td>The SQL query uses the ROW_NUMBER function to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5102</td>\n",
       "      <td>defense operations</td>\n",
       "      <td>Defense data on military innovation, peacekeep...</td>\n",
       "      <td>basic SQL</td>\n",
       "      <td>basic SQL with a simple select statement</td>\n",
       "      <td>analytics and reporting</td>\n",
       "      <td>generating reports, dashboards, and analytical...</td>\n",
       "      <td>What is the total spending on humanitarian ass...</td>\n",
       "      <td>CREATE SCHEMA if not exists defense; CREATE TA...</td>\n",
       "      <td>SELECT SUM(spending) FROM defense.eu_humanitar...</td>\n",
       "      <td>This SQL query calculates the total spending o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5103</td>\n",
       "      <td>aquaculture</td>\n",
       "      <td>Aquatic farming data, fish stock management, o...</td>\n",
       "      <td>single join</td>\n",
       "      <td>only one join (specify inner, outer, cross)</td>\n",
       "      <td>analytics and reporting</td>\n",
       "      <td>generating reports, dashboards, and analytical...</td>\n",
       "      <td>What is the average water temperature for each...</td>\n",
       "      <td>CREATE TABLE SpeciesWaterTemp (SpeciesID int, ...</td>\n",
       "      <td>SELECT SpeciesName, AVG(WaterTemp) as AvgTemp ...</td>\n",
       "      <td>This query calculates the average water temper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5104</td>\n",
       "      <td>nonprofit operations</td>\n",
       "      <td>Donation records, program outcomes, volunteer ...</td>\n",
       "      <td>basic SQL</td>\n",
       "      <td>basic SQL with a simple select statement</td>\n",
       "      <td>data manipulation</td>\n",
       "      <td>inserting, updating, or deleting records</td>\n",
       "      <td>Delete a program's outcome data</td>\n",
       "      <td>CREATE TABLE Program_Outcomes (id INT, program...</td>\n",
       "      <td>DELETE FROM Program_Outcomes WHERE program_id ...</td>\n",
       "      <td>This query removes the record with a program_i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5105</td>\n",
       "      <td>public transportation</td>\n",
       "      <td>Extensive data on route planning, fare collect...</td>\n",
       "      <td>basic SQL</td>\n",
       "      <td>basic SQL with a simple select statement</td>\n",
       "      <td>analytics and reporting</td>\n",
       "      <td>generating reports, dashboards, and analytical...</td>\n",
       "      <td>Find the total fare collected from passengers ...</td>\n",
       "      <td>CREATE TABLE bus_routes (route_name VARCHAR(50...</td>\n",
       "      <td>SELECT SUM(fare) FROM bus_routes WHERE route_n...</td>\n",
       "      <td>This SQL query calculates the total fare colle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5106</td>\n",
       "      <td>real estate</td>\n",
       "      <td>Real estate data on inclusive housing policies...</td>\n",
       "      <td>basic SQL</td>\n",
       "      <td>basic SQL with a simple select statement</td>\n",
       "      <td>analytics and reporting</td>\n",
       "      <td>generating reports, dashboards, and analytical...</td>\n",
       "      <td>What is the average property size in inclusive...</td>\n",
       "      <td>CREATE TABLE Inclusive_Housing (Property_ID IN...</td>\n",
       "      <td>SELECT AVG(Property_Size) FROM Inclusive_Housi...</td>\n",
       "      <td>The SQL query calculates the average property ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                 domain  \\\n",
       "0  5097               forestry   \n",
       "1  5098       defense industry   \n",
       "2  5099         marine biology   \n",
       "3  5100     financial services   \n",
       "4  5101                 energy   \n",
       "5  5102     defense operations   \n",
       "6  5103            aquaculture   \n",
       "7  5104   nonprofit operations   \n",
       "8  5105  public transportation   \n",
       "9  5106            real estate   \n",
       "\n",
       "                                  domain_description    sql_complexity  \\\n",
       "0  Comprehensive data on sustainable forest manag...       single join   \n",
       "1  Defense contract data, military equipment main...       aggregation   \n",
       "2  Comprehensive data on marine species, oceanogr...         basic SQL   \n",
       "3  Detailed financial data including investment s...       aggregation   \n",
       "4  Energy market data covering renewable energy s...  window functions   \n",
       "5  Defense data on military innovation, peacekeep...         basic SQL   \n",
       "6  Aquatic farming data, fish stock management, o...       single join   \n",
       "7  Donation records, program outcomes, volunteer ...         basic SQL   \n",
       "8  Extensive data on route planning, fare collect...         basic SQL   \n",
       "9  Real estate data on inclusive housing policies...         basic SQL   \n",
       "\n",
       "                          sql_complexity_description            sql_task_type  \\\n",
       "0        only one join (specify inner, outer, cross)  analytics and reporting   \n",
       "1  aggregation functions (COUNT, SUM, AVG, MIN, M...  analytics and reporting   \n",
       "2           basic SQL with a simple select statement  analytics and reporting   \n",
       "3  aggregation functions (COUNT, SUM, AVG, MIN, M...  analytics and reporting   \n",
       "4  window functions (e.g., ROW_NUMBER, LEAD, LAG,...  analytics and reporting   \n",
       "5           basic SQL with a simple select statement  analytics and reporting   \n",
       "6        only one join (specify inner, outer, cross)  analytics and reporting   \n",
       "7           basic SQL with a simple select statement        data manipulation   \n",
       "8           basic SQL with a simple select statement  analytics and reporting   \n",
       "9           basic SQL with a simple select statement  analytics and reporting   \n",
       "\n",
       "                           sql_task_type_description  \\\n",
       "0  generating reports, dashboards, and analytical...   \n",
       "1  generating reports, dashboards, and analytical...   \n",
       "2  generating reports, dashboards, and analytical...   \n",
       "3  generating reports, dashboards, and analytical...   \n",
       "4  generating reports, dashboards, and analytical...   \n",
       "5  generating reports, dashboards, and analytical...   \n",
       "6  generating reports, dashboards, and analytical...   \n",
       "7           inserting, updating, or deleting records   \n",
       "8  generating reports, dashboards, and analytical...   \n",
       "9  generating reports, dashboards, and analytical...   \n",
       "\n",
       "                                          sql_prompt  \\\n",
       "0  What is the total volume of timber sold by eac...   \n",
       "1  List all the unique equipment types and their ...   \n",
       "2  How many marine species are found in the South...   \n",
       "3  What is the total trade value and average pric...   \n",
       "4  Find the energy efficiency upgrades with the h...   \n",
       "5  What is the total spending on humanitarian ass...   \n",
       "6  What is the average water temperature for each...   \n",
       "7                    Delete a program's outcome data   \n",
       "8  Find the total fare collected from passengers ...   \n",
       "9  What is the average property size in inclusive...   \n",
       "\n",
       "                                         sql_context  \\\n",
       "0  CREATE TABLE salesperson (salesperson_id INT, ...   \n",
       "1  CREATE TABLE equipment_maintenance (equipment_...   \n",
       "2  CREATE TABLE marine_species (name VARCHAR(50),...   \n",
       "3  CREATE TABLE trade_history (id INT, trader_id ...   \n",
       "4  CREATE TABLE upgrades (id INT, cost FLOAT, typ...   \n",
       "5  CREATE SCHEMA if not exists defense; CREATE TA...   \n",
       "6  CREATE TABLE SpeciesWaterTemp (SpeciesID int, ...   \n",
       "7  CREATE TABLE Program_Outcomes (id INT, program...   \n",
       "8  CREATE TABLE bus_routes (route_name VARCHAR(50...   \n",
       "9  CREATE TABLE Inclusive_Housing (Property_ID IN...   \n",
       "\n",
       "                                                 sql  \\\n",
       "0  SELECT salesperson_id, name, SUM(volume) as to...   \n",
       "1  SELECT equipment_type, SUM(maintenance_frequen...   \n",
       "2  SELECT COUNT(*) FROM marine_species WHERE loca...   \n",
       "3  SELECT trader_id, stock, SUM(price * quantity)...   \n",
       "4  SELECT type, cost FROM (SELECT type, cost, ROW...   \n",
       "5  SELECT SUM(spending) FROM defense.eu_humanitar...   \n",
       "6  SELECT SpeciesName, AVG(WaterTemp) as AvgTemp ...   \n",
       "7  DELETE FROM Program_Outcomes WHERE program_id ...   \n",
       "8  SELECT SUM(fare) FROM bus_routes WHERE route_n...   \n",
       "9  SELECT AVG(Property_Size) FROM Inclusive_Housi...   \n",
       "\n",
       "                                     sql_explanation  \n",
       "0  Joins timber_sales and salesperson tables, gro...  \n",
       "1  This query groups the equipment_maintenance ta...  \n",
       "2  This query counts the number of marine species...  \n",
       "3  This query calculates the total trade value an...  \n",
       "4  The SQL query uses the ROW_NUMBER function to ...  \n",
       "5  This SQL query calculates the total spending o...  \n",
       "6  This query calculates the average water temper...  \n",
       "7  This query removes the record with a program_i...  \n",
       "8  This SQL query calculates the total fare colle...  \n",
       "9  The SQL query calculates the average property ...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataset.to_pandas()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d13c7869-9804-4fa9-869c-ecd1a5ed8355",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=1234).select(range(12500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52ab09f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_prompt(datum):\n",
    "    prompt = f\"\"\"\n",
    "<start_of_turn>user\n",
    "You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA: {datum[\"sql_context\"]}\n",
    "\n",
    "{datum[\"sql_prompt\"]}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "{datum[\"sql\"]}<end_of_turn>\n",
    "\"\"\".strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c098352b-80e4-4752-b643-9d02b19b5b31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove columns: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_explanation']\n"
     ]
    }
   ],
   "source": [
    "columns_to_remove = list(dataset.features)\n",
    "for k in ['sql', 'sql_context', 'sql_prompt']:\n",
    "    columns_to_remove.remove(k)\n",
    "print(f'remove columns: {columns_to_remove}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cef6d0dc-2817-431c-b58b-d1e53c8e0704",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sql_prompt', 'sql_context', 'sql', 'prompt'],\n",
       "    num_rows: 12500\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_column = [generate_prompt(datum) for datum in dataset]\n",
    "dataset = dataset.add_column(\"prompt\", prompt_column)\n",
    "dataset = dataset.remove_columns(columns_to_remove)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a0d1c61-d801-4318-b10e-f77334ec979c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sql_prompt', 'sql_context', 'sql', 'prompt'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sql_prompt', 'sql_context', 'sql', 'prompt'],\n",
       "        num_rows: 2500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c4565381-4339-4063-880a-996cae26568f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start_of_turn>user\n",
      "You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
      "SCHEMA: CREATE TABLE volunteers (id INT, name VARCHAR(50), reg_date DATE, location VARCHAR(30)); INSERT INTO volunteers (id, name, reg_date, location) VALUES (1, 'Alex', '2023-02-01', 'urban'), (2, 'Bella', '2023-01-15', 'rural'), (3, 'Charlie', '2023-03-05', 'suburban'), (4, 'Diana', '2022-07-20', 'rural'), (5, 'Eli', '2022-09-01', 'urban');\n",
      "\n",
      "What is the total number of volunteers from urban areas who have volunteered in the last 6 months?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "SELECT COUNT(*) FROM volunteers WHERE location = 'urban' AND reg_date >= DATEADD(month, -6, GETDATE());<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][345]['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c609924d-0a7a-4b94-8bd1-70ee3b48b2f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a7ad0eec034a6991bda0a185cb4c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f974f0f8c9fc4434a742b65ab5a9693b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data uploaded to:\n",
      "s3://sagemaker-us-east-1-395271362395/datasets/text-to-sql/train_dataset.json\n",
      "https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-395271362395/?region=us-east-1&prefix=datasets/text-to-sql/\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3 using our SageMaker session\n",
    "training_input_path = f\"s3://{sess.default_bucket()}/datasets/text-to-sql\"\n",
    "\n",
    "# save datasets to s3\n",
    "dataset[\"train\"].to_json(f\"{training_input_path}/train_dataset.json\", orient=\"records\")\n",
    "dataset[\"test\"].to_json(f\"{training_input_path}/test_dataset.json\", orient=\"records\")\n",
    "\n",
    "print(f\"Training data uploaded to:\")\n",
    "print(f\"{training_input_path}/train_dataset.json\")\n",
    "print(\n",
    "    f\"https://s3.console.aws.amazon.com/s3/buckets/{sess.default_bucket()}/?region={sess.boto_region_name}&prefix={training_input_path.split('/', 3)[-1]}/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f077492-56f3-482b-a250-68e270c2a2d4",
   "metadata": {},
   "source": [
    "# 3. Fine-Tune Gemma2 with QLoRA on Amazon Sagemaker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "13ef4988-2c82-479c-9e8b-88284b2d0ed3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field, fields, asdict\n",
    "from trl import SFTConfig\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PartialTrainingArguments:\n",
    "    num_train_epochs: int\n",
    "    per_device_train_batch_size: int\n",
    "    gradient_accumulation_steps: int\n",
    "    gradient_checkpointing: bool\n",
    "    optim: str\n",
    "    logging_steps: int\n",
    "    save_strategy: int\n",
    "    learning_rate: float\n",
    "    bf16: bool\n",
    "    tf32: bool\n",
    "    max_grad_norm: float\n",
    "    warmup_ratio: float\n",
    "    lr_scheduler_type: str\n",
    "    report_to: str\n",
    "    output_dir: str\n",
    "    # SFTrainer Config\n",
    "    dataset_text_field: str\n",
    "    max_seq_length: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Hyperparameters(PartialTrainingArguments):\n",
    "    # path where sagemaker will save training dataset\n",
    "    dataset_path: str\n",
    "    model_id: str\n",
    "    use_qlora: bool\n",
    "    merge_adapters: bool\n",
    "\n",
    "    def to_dict(self):\n",
    "        return asdict(self)\n",
    "\n",
    "\n",
    "# Validate training arguments\n",
    "training_args_fields = {field.name for field in fields(SFTConfig)}\n",
    "partial_training_args_fields = {\n",
    "    field.name for field in fields(PartialTrainingArguments)\n",
    "}\n",
    "is_subset = partial_training_args_fields.issubset(training_args_fields)\n",
    "assert (\n",
    "    is_subset\n",
    "), \"All fields in PartialTrainingArguments should be in SFTConfig\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "20facf95-1c8c-40fc-9388-2d841ed7f680",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_train_epochs': 3, 'per_device_train_batch_size': 1, 'gradient_accumulation_steps': 4, 'gradient_checkpointing': True, 'optim': 'paged_adamw_8bit', 'logging_steps': 10, 'save_strategy': 'epoch', 'learning_rate': 0.0002, 'bf16': True, 'tf32': True, 'max_grad_norm': 0.3, 'warmup_ratio': 0.03, 'lr_scheduler_type': 'constant', 'report_to': 'tensorboard', 'output_dir': '/tmp/tun', 'dataset_text_field': 'prompt', 'max_seq_length': 1024, 'dataset_path': '/opt/ml/input/data/training/train_dataset.json', 'model_id': 'google/gemma-2-9b', 'use_qlora': True, 'merge_adapters': True}\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = Hyperparameters(\n",
    "    ### SagemakerArguments ###\n",
    "    dataset_path=\"/opt/ml/input/data/training/train_dataset.json\",\n",
    "    model_id=\"google/gemma-2-9b\",\n",
    "    use_qlora=True,\n",
    "    merge_adapters=True,\n",
    "    ### TrainingArguments ###\n",
    "    num_train_epochs=3,  # number of training epochs\n",
    "    per_device_train_batch_size=1,  # batch size per device during training\n",
    "    gradient_accumulation_steps=4,  # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,  # use gradient checkpointing to save memory\n",
    "    optim=\"paged_adamw_8bit\",  # use paged_adamw_8bit optimizer\n",
    "    logging_steps=10,  # log every 10 steps\n",
    "    save_strategy=\"epoch\",  # save checkpoint every epoch\n",
    "    learning_rate=2e-4,  # learning rate, based on QLoRA paper\n",
    "    bf16=True,  # use bfloat16 precision\n",
    "    tf32=True,  # use tf32 precision\n",
    "    max_grad_norm=0.3,  # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,  # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",  # use constant learning rate scheduler\n",
    "    report_to=\"tensorboard\",  # report metrics to tensorboard\n",
    "    output_dir=\"/tmp/tun\",  # Temporary output directory for model checkpoints\n",
    "    ### SFTrainer Config ###\n",
    "    dataset_text_field=\"prompt\",  # dataset prompt key\n",
    "    max_seq_length=1024,\n",
    ").to_dict()\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dc1d9dd3-abdd-4aeb-a70b-4859b4295c51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f\"gemma-9b-it-text-to-sql\"\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point=\"run_sft.py\",  # train script\n",
    "    source_dir=\"./scripts\",  # directory which includes all the files needed for training\n",
    "    instance_type=\"ml.g5.8xlarge\",  # instances type used for the training job\n",
    "    instance_count=1,  # the number of instances used for training\n",
    "    max_run=2\n",
    "    * 24\n",
    "    * 3600,  # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name=job_name,  # the name of the training job\n",
    "    role=role,  # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size=300,  # the size of the EBS volume in GB\n",
    "    transformers_version=\"4.36\",  # the transformers version used in the training job\n",
    "    pytorch_version=\"2.1\",  # the pytorch_version version used in the training job\n",
    "    py_version=\"py310\",  # the python version used in the training job\n",
    "    hyperparameters=hyperparameters,  # the hyperparameters passed to the training job\n",
    "    disable_output_compression=True,  # not compress output to save training time and cost\n",
    "    environment={\n",
    "        \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\",  # set env variable to cache models in /tmp\n",
    "        \"HF_TOKEN\": \"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\",  # huggingface token to access gated models, e.g. llama 2\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353ae3dd-75f1-4d65-96ea-836df2b0f001",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: gemma-9b-it-text-to-sql-2024-07-18-05-52-35-160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-18 05:52:35 Starting - Starting the training job...\n",
      "2024-07-18 05:52:51 Starting - Preparing the instances for training...\n",
      "2024-07-18 05:53:25 Downloading - Downloading input data...\n",
      "2024-07-18 05:53:40 Downloading - Downloading the training image.....................\n",
      "2024-07-18 05:57:08 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-07-18 05:57:33,139 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-07-18 05:57:33,156 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-07-18 05:57:33,167 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-07-18 05:57:33,169 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-07-18 05:57:34,619 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers@ git+https://github.com/huggingface/transformers.git (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mCloning https://github.com/huggingface/transformers.git to /tmp/pip-install-ljgbbtlz/transformers_e595556eea0746fdb953bd60bc09faaf\u001b[0m\n",
      "\u001b[34mRunning command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-install-ljgbbtlz/transformers_e595556eea0746fdb953bd60bc09faaf\u001b[0m\n",
      "\u001b[34mResolved https://github.com/huggingface/transformers.git to commit 1c37e8c1a6274e6e87b45c6319eb190757214c2a\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting datasets==2.20.0 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.32.1 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.32.1-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting evaluate==0.4.2 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes==0.43.1 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting trl==0.9.6 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting peft==0.11.1 (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting flash-attn==2.6.1 (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading flash_attn-2.6.1.tar.gz (2.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.6/2.6 MB 85.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets==2.20.0->-r requirements.txt (line 2)) (3.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.20.0->-r requirements.txt (line 2)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.20.0->-r requirements.txt (line 2)) (15.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.20.0->-r requirements.txt (line 2)) (0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.20.0->-r requirements.txt (line 2)) (0.3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.20.0->-r requirements.txt (line 2)) (2.1.2)\u001b[0m\n",
      "\u001b[34mCollecting requests>=2.32.2 (from datasets==2.20.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting tqdm>=4.66.3 (from datasets==2.20.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.6/57.6 kB 10.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.20.0->-r requirements.txt (line 2)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.20.0->-r requirements.txt (line 2)) (0.70.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets==2.20.0->-r requirements.txt (line 2)) (2023.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.20.0->-r requirements.txt (line 2)) (3.9.3)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub>=0.21.2 (from datasets==2.20.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.23.5-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.20.0->-r requirements.txt (line 2)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.20.0->-r requirements.txt (line 2)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.32.1->-r requirements.txt (line 3)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.32.1->-r requirements.txt (line 3)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.32.1->-r requirements.txt (line 3)) (0.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl==0.9.6->-r requirements.txt (line 6)) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.6.1->-r requirements.txt (line 8)) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers@ git+https://github.com/huggingface/transformers.git->-r requirements.txt (line 1)) (2023.12.25)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.20,>=0.19 (from transformers@ git+https://github.com/huggingface/transformers.git->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 2)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 2)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 2)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 2)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 2)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets==2.20.0->-r requirements.txt (line 2)) (4.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets==2.20.0->-r requirements.txt (line 2)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets==2.20.0->-r requirements.txt (line 2)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets==2.20.0->-r requirements.txt (line 2)) (1.26.18)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets==2.20.0->-r requirements.txt (line 2)) (2023.7.22)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.32.1->-r requirements.txt (line 3)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.32.1->-r requirements.txt (line 3)) (3.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.32.1->-r requirements.txt (line 3)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.9.6->-r requirements.txt (line 6)) (0.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.9.6->-r requirements.txt (line 6)) (13.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.9.6->-r requirements.txt (line 6)) (1.6.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.20.0->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.20.0->-r requirements.txt (line 2)) (2023.3.post1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.20.0->-r requirements.txt (line 2)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.20.0->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.9.6->-r requirements.txt (line 6)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.9.6->-r requirements.txt (line 6)) (2.16.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.32.1->-r requirements.txt (line 3)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.32.1->-r requirements.txt (line 3)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.9.6->-r requirements.txt (line 6)) (0.1.0)\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.20.0-py3-none-any.whl (547 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 547.8/547.8 kB 55.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.32.1-py3-none-any.whl (314 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 314.1/314.1 kB 35.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.1/84.1 kB 16.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.8/119.8 MB 21.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading trl-0.9.6-py3-none-any.whl (245 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 245.8/245.8 kB 25.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading peft-0.11.1-py3-none-any.whl (251 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 251.6/251.6 kB 32.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.23.5-py3-none-any.whl (402 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 402.8/402.8 kB 37.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading requests-2.32.3-py3-none-any.whl (64 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.9/64.9 kB 12.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 96.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.3/78.3 kB 15.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: flash-attn, transformers\u001b[0m\n",
      "\u001b[34mBuilding wheel for flash-attn (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for flash-attn (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for flash-attn: filename=flash_attn-2.6.1-cp310-cp310-linux_x86_64.whl size=198134157 sha256=103e9c56d7f15d32f86f4c50e264139e31396cc5db1565cd6c951bf90c27fc28\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/91/6a/38/f0faa036b4ac73a73247386f1ab1bb4cb4f6e72e6861a779f1\u001b[0m\n",
      "\u001b[34mBuilding wheel for transformers (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for transformers (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for transformers: filename=transformers-4.43.0.dev0-py3-none-any.whl size=9391970 sha256=9f047da104ecdaa0fd0d9d976f2a48fd5b7913aae45c70651405887cc58bb3f1\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-nfqxn7z5/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\u001b[0m\n",
      "\u001b[34mSuccessfully built flash-attn transformers\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tqdm, requests, huggingface-hub, tokenizers, flash-attn, bitsandbytes, accelerate, transformers, datasets, trl, peft, evaluate\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tqdm\u001b[0m\n",
      "\u001b[34mFound existing installation: tqdm 4.66.1\u001b[0m\n",
      "\u001b[34mUninstalling tqdm-4.66.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tqdm-4.66.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: requests\u001b[0m\n",
      "\u001b[34mFound existing installation: requests 2.31.0\u001b[0m\n",
      "\u001b[34mUninstalling requests-2.31.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled requests-2.31.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.20.3\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.20.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.20.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tokenizers\u001b[0m\n",
      "\u001b[34mFound existing installation: tokenizers 0.15.1\u001b[0m\n",
      "\u001b[34mUninstalling tokenizers-0.15.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tokenizers-0.15.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: flash-attn\u001b[0m\n",
      "\u001b[34mFound existing installation: flash-attn 2.3.6\u001b[0m\n",
      "\u001b[34mUninstalling flash-attn-2.3.6:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled flash-attn-2.3.6\u001b[0m\n",
      "\u001b[34mAttempting uninstall: bitsandbytes\u001b[0m\n",
      "\u001b[34mFound existing installation: bitsandbytes 0.42.0\u001b[0m\n",
      "\u001b[34mUninstalling bitsandbytes-0.42.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled bitsandbytes-0.42.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.25.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.25.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.25.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.36.0\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.36.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.36.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.15.0\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.15.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.15.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: trl\u001b[0m\n",
      "\u001b[34mFound existing installation: trl 0.7.4\u001b[0m\n",
      "\u001b[34mUninstalling trl-0.7.4:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled trl-0.7.4\u001b[0m\n",
      "\u001b[34mAttempting uninstall: peft\u001b[0m\n",
      "\u001b[34mFound existing installation: peft 0.7.1\u001b[0m\n",
      "\u001b[34mUninstalling peft-0.7.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled peft-0.7.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: evaluate\u001b[0m\n",
      "\u001b[34mFound existing installation: evaluate 0.4.1\u001b[0m\n",
      "\u001b[34mUninstalling evaluate-0.4.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled evaluate-0.4.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.32.1 bitsandbytes-0.43.1 datasets-2.20.0 evaluate-0.4.2 flash-attn-2.6.1 huggingface-hub-0.23.5 peft-0.11.1 requests-2.32.3 tokenizers-0.19.1 tqdm-4.66.4 transformers-4.43.0.dev0 trl-0.9.6\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.3.2 -> 24.1.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-07-18 05:58:15,807 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-07-18 05:58:15,807 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-07-18 05:58:15,844 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-07-18 05:58:15,874 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-07-18 05:58:15,905 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-07-18 05:58:15,918 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.8xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bf16\": true,\n",
      "        \"dataset_path\": \"/opt/ml/input/data/training/train_dataset.json\",\n",
      "        \"dataset_text_field\": \"prompt\",\n",
      "        \"gradient_accumulation_steps\": 4,\n",
      "        \"gradient_checkpointing\": true,\n",
      "        \"learning_rate\": 0.0002,\n",
      "        \"logging_steps\": 10,\n",
      "        \"lr_scheduler_type\": \"constant\",\n",
      "        \"max_grad_norm\": 0.3,\n",
      "        \"max_seq_length\": 1024,\n",
      "        \"merge_adapters\": true,\n",
      "        \"model_id\": \"google/gemma-2-9b\",\n",
      "        \"num_train_epochs\": 3,\n",
      "        \"optim\": \"paged_adamw_8bit\",\n",
      "        \"output_dir\": \"/tmp/tun\",\n",
      "        \"per_device_train_batch_size\": 1,\n",
      "        \"report_to\": \"tensorboard\",\n",
      "        \"save_strategy\": \"epoch\",\n",
      "        \"tf32\": true,\n",
      "        \"use_qlora\": true,\n",
      "        \"warmup_ratio\": 0.03\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.8xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": false,\n",
      "    \"job_name\": \"gemma-9b-it-text-to-sql-2024-07-18-05-52-35-160\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-395271362395/gemma-9b-it-text-to-sql-2024-07-18-05-52-35-160/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_sft\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 32,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.8xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.8xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_sft.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/training/train_dataset.json\",\"dataset_text_field\":\"prompt\",\"gradient_accumulation_steps\":4,\"gradient_checkpointing\":true,\"learning_rate\":0.0002,\"logging_steps\":10,\"lr_scheduler_type\":\"constant\",\"max_grad_norm\":0.3,\"max_seq_length\":1024,\"merge_adapters\":true,\"model_id\":\"google/gemma-2-9b\",\"num_train_epochs\":3,\"optim\":\"paged_adamw_8bit\",\"output_dir\":\"/tmp/tun\",\"per_device_train_batch_size\":1,\"report_to\":\"tensorboard\",\"save_strategy\":\"epoch\",\"tf32\":true,\"use_qlora\":true,\"warmup_ratio\":0.03}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_sft.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.8xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.8xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.8xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.8xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_sft\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=32\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-395271362395/gemma-9b-it-text-to-sql-2024-07-18-05-52-35-160/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.8xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/training/train_dataset.json\",\"dataset_text_field\":\"prompt\",\"gradient_accumulation_steps\":4,\"gradient_checkpointing\":true,\"learning_rate\":0.0002,\"logging_steps\":10,\"lr_scheduler_type\":\"constant\",\"max_grad_norm\":0.3,\"max_seq_length\":1024,\"merge_adapters\":true,\"model_id\":\"google/gemma-2-9b\",\"num_train_epochs\":3,\"optim\":\"paged_adamw_8bit\",\"output_dir\":\"/tmp/tun\",\"per_device_train_batch_size\":1,\"report_to\":\"tensorboard\",\"save_strategy\":\"epoch\",\"tf32\":true,\"use_qlora\":true,\"warmup_ratio\":0.03},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.8xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"gemma-9b-it-text-to-sql-2024-07-18-05-52-35-160\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-395271362395/gemma-9b-it-text-to-sql-2024-07-18-05-52-35-160/source/sourcedir.tar.gz\",\"module_name\":\"run_sft\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.8xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.8xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_sft.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--bf16\",\"True\",\"--dataset_path\",\"/opt/ml/input/data/training/train_dataset.json\",\"--dataset_text_field\",\"prompt\",\"--gradient_accumulation_steps\",\"4\",\"--gradient_checkpointing\",\"True\",\"--learning_rate\",\"0.0002\",\"--logging_steps\",\"10\",\"--lr_scheduler_type\",\"constant\",\"--max_grad_norm\",\"0.3\",\"--max_seq_length\",\"1024\",\"--merge_adapters\",\"True\",\"--model_id\",\"google/gemma-2-9b\",\"--num_train_epochs\",\"3\",\"--optim\",\"paged_adamw_8bit\",\"--output_dir\",\"/tmp/tun\",\"--per_device_train_batch_size\",\"1\",\"--report_to\",\"tensorboard\",\"--save_strategy\",\"epoch\",\"--tf32\",\"True\",\"--use_qlora\",\"True\",\"--warmup_ratio\",\"0.03\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BF16=true\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_PATH=/opt/ml/input/data/training/train_dataset.json\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_TEXT_FIELD=prompt\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=4\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_CHECKPOINTING=true\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0002\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=10\u001b[0m\n",
      "\u001b[34mSM_HP_LR_SCHEDULER_TYPE=constant\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_GRAD_NORM=0.3\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SEQ_LENGTH=1024\u001b[0m\n",
      "\u001b[34mSM_HP_MERGE_ADAPTERS=true\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=google/gemma-2-9b\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=3\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIM=paged_adamw_8bit\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/tmp/tun\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_REPORT_TO=tensorboard\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_TF32=true\u001b[0m\n",
      "\u001b[34mSM_HP_USE_QLORA=true\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_RATIO=0.03\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 run_sft.py --bf16 True --dataset_path /opt/ml/input/data/training/train_dataset.json --dataset_text_field prompt --gradient_accumulation_steps 4 --gradient_checkpointing True --learning_rate 0.0002 --logging_steps 10 --lr_scheduler_type constant --max_grad_norm 0.3 --max_seq_length 1024 --merge_adapters True --model_id google/gemma-2-9b --num_train_epochs 3 --optim paged_adamw_8bit --output_dir /tmp/tun --per_device_train_batch_size 1 --report_to tensorboard --save_strategy epoch --tf32 True --use_qlora True --warmup_ratio 0.03\u001b[0m\n",
      "\u001b[34m2024-07-18 05:58:15,919 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2024-07-18 05:58:15,919 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 10000 examples [00:00, 193127.48 examples/s]\u001b[0m\n",
      "\u001b[34mUsing QLoRA\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  12%|█▎        | 1/8 [00:11<01:23, 11.95s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 2/8 [00:24<01:12, 12.06s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  38%|███▊      | 3/8 [00:35<00:59, 11.88s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 4/8 [00:48<00:48, 12.12s/it]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\"training\": training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddac057-fcf0-4749-a95f-da66813e7ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
