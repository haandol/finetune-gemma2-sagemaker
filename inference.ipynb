{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8539f22a-579b-43bb-a63a-c60eb55a8b14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install -q -U bitsandbytes\n",
    "!pip3 install -q -U peft\n",
    "!pip3 install -q -U trl\n",
    "!pip3 install -q -U accelerate\n",
    "!pip3 install -q -U datasets\n",
    "!pip3 install -q -U transformers\n",
    "!pip install -q -U \"huggingface_hub[cli]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a791478-b045-4f7c-ade2-086b22c5142c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "630aa01f41a24fb9998e1575aed5cce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0f17a90-7e35-41cd-bb93-b7af359e85e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::395271362395:role/SagemakerStudioDemoSagema-SageMakerExecutionRole78-5I33I083KE6P\n",
      "sagemaker bucket: sagemaker-us-east-1-395271362395\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "30e6b6a5-7424-4cf9-b0ba-7276ee5e13aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "downloader = S3Downloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fe893173-a55b-479d-a1c5-ece1c8ffba1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./model/config.json',\n",
       " './model/generation_config.json',\n",
       " './model/model-00001-of-00010.safetensors',\n",
       " './model/model-00002-of-00010.safetensors',\n",
       " './model/model-00003-of-00010.safetensors',\n",
       " './model/model-00004-of-00010.safetensors',\n",
       " './model/model-00005-of-00010.safetensors',\n",
       " './model/model-00006-of-00010.safetensors',\n",
       " './model/model-00007-of-00010.safetensors',\n",
       " './model/model-00008-of-00010.safetensors',\n",
       " './model/model-00009-of-00010.safetensors',\n",
       " './model/model-00010-of-00010.safetensors',\n",
       " './model/model.safetensors.index.json',\n",
       " './model/special_tokens_map.json',\n",
       " './model/tokenizer.json',\n",
       " './model/tokenizer.model',\n",
       " './model/tokenizer_config.json']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output = 's3://sagemaker-us-east-1-395271362395/gemma-9b-text-to-sql-2024-07-18-08-47-19-847/output/model'\n",
    "downloader.download(model_output, './model', sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0ede42b-bbe3-4b11-ac9a-01f7af6fc4d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "try:\n",
    "    del model\n",
    "    del tokenizer\n",
    "except:\n",
    "    pass\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee941969-e03d-4858-8b4d-6e17893a6eaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul 19 01:22:45 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A10G                    On  | 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   32C    P0              65W / 300W |    334MiB / 23028MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     13038      C   ...conda3/envs/pytorch_p310/bin/python      326MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "967a9a7f-d9c6-481b-9b5d-fc65a24a4d14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e83e910857444dbbe2a50f04ea35b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "#model_id = 'google/gemma-2-9b'\n",
    "model_id = './model'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    add_eos_token=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c90f9be-5d7e-48a4-a11d-41ddff572df7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a99e91c5-301c-4024-b30b-9b801d8f9294",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_prompt(schema: str, query: str):\n",
    "    prompt = f\"\"\"\n",
    "<start_of_turn>user\n",
    "You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA: {schema}\n",
    "\n",
    "{query}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\".lstrip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74c8b7ba-7e52-4980-915c-1328dd3ff0bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_completion(schema, query, model, tokenizer) -> str:\n",
    "    prompt = generate_prompt(schema, query)\n",
    "    encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    model_inputs = encodeds.to(device)\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return (decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50c321c0-c98c-471e-abdf-fce897b19c2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
      "SCHEMA: CREATE TABLE head (name VARCHAR, born_state VARCHAR, age VARCHAR);\n",
      "\n",
      "List the name, born state and age of the heads of departments ordered by age.\n",
      "model\n",
      " head FROM head JOIN ( SELECT head FROM head WHERE department LIKE '%head%') AS heads_of_departments ON heads_of_departments.born_state IS NOT NULL;\n",
      "model\n",
      "SELECT name, born_state, age FROM heads_of_departments;\n",
      "model\n",
      "SELECT name, born_state, age FROM ( SELECT data.name FROM data JOIN (SELECT * FROM data WHERE department LIKE '%head%') AS head FROM data WHERE head EXISTS ) AS heads_of_departments;\n",
      "model\n",
      "SELECT name, born_state, age FROM heads_of_departments;\n",
      "model\n",
      "SELECT name, born_state, age FROM (SELECT data.name FROM data JOIN (SELECT * FROM data WHERE department LIKE '%head%') AS head FROM data WHERE head EXISTS) AS heads_of_departments;\n",
      "model\n",
      "SELECT name, born_state, age FROM heads_of_departments;\n",
      "model\n",
      "SELECT name, born_state, age FROM (SELECT name FROM heads_of_departments) AS heads;\n",
      "model\n",
      "SELECT name FROM heads;\n",
      "model\n",
      "SELECT age FROM heads;\n",
      "model\n",
      "SELECT name, age FROM heads;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SELECT name, born_state, age FROM head ORDER BY age\n",
    "\n",
    "result = get_completion(\n",
    "    schema=\"CREATE TABLE head (name VARCHAR, born_state VARCHAR, age VARCHAR);\",\n",
    "    query=\"List the name, born state and age of the heads of departments ordered by age.\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be9740e3-dfe2-4546-a99b-6ead6d81b129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
      "SCHEMA: CREATE TABLE head (name VARCHAR, born_state VARCHAR);\n",
      "\n",
      "What are the names of the heads who are born outside the California state?\n",
      "model\n",
      "SELECT name FROM head WHERE born_state!= 'California';\n",
      "model\n",
      "SELECT name FROM head WHERE birth_location != 'California';\n",
      "model\n",
      "SELECT name FROM head WHERE state != 'California';\n",
      "model\n",
      "SELECT name FROM head WHERE state != 'California';\n",
      "model\n",
      "SELECT name FROM head WHERE birth_date >= CURRENT_DATE - INTERVAL (SELECT EXTRACT(CURRENT_DATE FROM head) - birth_date) * 10);\n",
      "model\n",
      "SELECT name FROM head WHERE birth_date <= CURRENT_DATE + INTERVAL '10 years';\n",
      "model\n",
      "SELECT name FROM head WHERE birth_date >= CURRENT_DATE - INTERVAL '20 years';\n",
      "model\n",
      "SELECT name FROM head WHERE birth_date >= CURRENT_DATE - INTERVAL '30 years';\n",
      "model\n",
      "SELECT name FROM head WHERE birth_date >= CURRENT_DATE - INTERVAL '50 years';\n",
      "model\n",
      "SELECT name FROM head WHERE birth_date >= CURRENT_DATE - INTERVAL '60 years';\n",
      "model\n",
      "SELECT name FROM head WHERE birth_date >= CURRENT_DATE - INTERVAL '70 years';\n",
      "model\n",
      "SELECT name FROM head WHERE birth_date >=\n"
     ]
    }
   ],
   "source": [
    "# SELECT name FROM head WHERE born_state <> 'California'\n",
    "\n",
    "result = get_completion(\n",
    "    schema=\"CREATE TABLE head (name VARCHAR, born_state VARCHAR);\",\n",
    "    query=\"What are the names of the heads who are born outside the California state?\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda0f8a5-cab4-4152-9354-b1f287140483",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
